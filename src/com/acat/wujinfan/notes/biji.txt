1，幂等
  推荐分布式锁（乐观锁）。
  场景
  <1>，一个订单不允许被多次支付（包括并发状态下不允许被多个人同时支付）
      下单前对订单状态（status字段）校验，对订单加上乐观锁（加上一个字段lock），只有加锁成功的人才能进行支付。
      或者针对每个订单生成唯一支付日志，保证一个未支付的订单只允许被一个线程支付。
  <2>，库存扣减，不允许超卖。
      需要考虑场景，在c端展示层，读取缓存的方式，如果库存扣减了，消息异步更新缓存。
      在对库存更改的时候，使用分布式锁，锁住某个产品id的库存，只允许一个线程去更改。

2，redis集群搭建。
   基于redis 3.0集群模式，多个master节点根据hash分布在16384槽上，每个master节点挂靠多个slave节点。
集群是好多个redis一起工作的，如果为了保证集群不是那么容易挂掉，所以呢，理论上就应该给集群中的每个节点至少一个slave redis节点。

3，redis集群模式：
<1>standalone类型架构，单节点结构(非集群模式)。
 缺点：单节点，存储空间和并发访问能力有很有限，很容易发生缓存穿透，流量直接打入db。

<2>redis主从，一个master挂着多个slave，
 优点：master一般只接受写入流量，slave负责读取，提高了负载能力（主从复制是乐观复制，当客户端发送写执行给主，主执行完立即将结果返回客户端，并异步的把命令发送给从，从而不影响性能）。
 缺点：
A,Redis不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。
B,主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。
C,主从机器都是全量备份的数据（浪费内存），单机需要更大内存，存储空间受限，不易扩容。

<3>哨兵模式：Redis 2.8中提供了哨兵工具来实现自动化的系统监控和故障恢复功能，哨兵的作用就是监控redis主、从节点是否正常运行，主出现故障自动将从节点转换为主节点。
哨兵的作用就是监控Redis系统的运行状况。它的功能包括以下两个
（1）监控主节点和从节点是否正常运行。
（2）主节点出现故障时自动将从节点转换为主节点。
 备注：哨兵也是集群部署，集群初始化时配置。

 哨兵工作原理：
哨兵(sentinel) 是一个分布式系统,你可以在一个架构中运行多个哨兵(sentinel) 进程,这些进程使用流言协议(gossipprotocols)来接收关于Master是否下线的信息,并使用投票协议(agreement protocols)来决定是否执行自动故障迁移,以及选择哪个Slave作为新的Master。
每个哨兵(sentinel) 会向其它哨兵(sentinel)、master、slave定时发送消息,以确认对方是否”活”着,如果发现对方在指定时间(可配置)内未回应,则暂时认为对方已挂(所谓的”主观认为宕机” Subjective Down,简称sdown)。若“哨兵群”中的多数sentinel,都报告某一master没响应,系统才认为该master"彻底死亡"(即:客观上的真正down机,Objective Down,简称odown),通过一定的vote算法,从剩下的slave节点中,选一台提升为master,然后自动修改相关配置.
虽然哨兵(sentinel) 释出为一个单独的可执行文件 redis-sentinel ,但实际上它只是一个运行在特殊模式下的 Redis 服务器，你可以在启动一个普通 Redis 服务器时通过给定 --sentinel 选项来启动哨兵(sentinel).哨兵(sentinel) 的一些设计思路和zookeeper非常类似

 优点：哨兵集群模式是基于主从模式的，所有主从的优点，哨兵模式同样具有，主从可以切换，故障可以转移，系统可靠性更高。
 缺点：
主从机器都是全量备份的数据（浪费内存），单机需要更大内存，存储空间受限，不易扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。
哨兵模式还存在脑裂问题
Redis 哨兵模式脑裂：master所在机器突然脱离了正常的网络，跟其他slave机器不能连接，但是实际上master还运行着
此时哨兵可能就会认为master宕机了，然后开启选举，将其他slave切换成了master，这个时候，集群里就会有两个master，也就是所谓的脑裂

解决方案：
min-slaves-to-write 1
min-slaves-max-lag 10
要求至少有1个slave，数据复制和同步的延迟不能超过10秒，如果说一旦所有的slave，数据复制和同步的延迟都超过了10秒钟
那么这个时候，master就不会再接收任何请求了上面两个配置可以减少异步复制和脑裂导致的数据丢失
上面的配置就确保了，如果跟任何一个slave丢了连接，在10秒后发现没有slave给自己ack，那么就拒绝新的写请求，因此在脑裂场景下，最多就丢失10秒的数据。
链接：https://www.jianshu.com/p/f6ceae73c7ae

<4>redis 3.0 cluster：分布式存储。即每台redis存储不同的内容，共有16384个slot。每个redis分得一些slot，hash_slot = crc16(key) mod 16384 找到对应slot，键是可用键，集群至少需要3主3从，且每个实例使用不同的配置文件，主从不用配置，集群会自己选。
备注：它们之间通过互相的ping-pong判断是否节点可以连接上。如果有一半以上的节点去ping一个节点的时候没有回应，集群就认为这个节点宕机了，然后去连接它的备用节点。如果某个节点和所有从节点全部挂掉，我们集群就进入faill状态。还有就是如果有一半以上的主节点宕机，那么我们集群同样进入发力了状态。这就是我们的redis的投票机制。

优点：具备哨兵模式的优点，数据分散存储，内存利用率更加高，支持在线扩容。
缺点：如果某个slot上面的master和slave都挂掉，就会出现集群不可用。

备注：在Redis Cluster3.0动态扩容时，新增的Master节点是没有数据的，主节点如果没有slots的话，存取数据就都不会被选中，需要手动把slot及其中数据迁移到新增的Master中（参考Redis集群官方中文教程），操作指令支持节点重新洗牌（调增slot对应的数据）。

<5>Jedis sharding集群
Redis Sharding可以说是在Redis cluster出来之前业界普遍的采用方式，其主要思想是采用hash算法将存储数据的key进行hash散列，这样特定的key会被定为到特定的节点上（采用一致性哈希算法，将key和节点name同时hashing，然后进行映射匹配）
<6>中间件代理
常见中间件：
Twemproxy
Codis
nginx


参考：https://blog.csdn.net/qq_35152037/article/details/84583573，https://mp.weixin.qq.com/s/a4JeJgSjxLSxOwczHStt0w



3.0 cluster那么这个集群是如何判断是否有某个节点挂掉了呢？
首先要说的是，每一个节点都存有这个集群所有主节点以及从节点的信息。
它们之间通过互相的ping-pong判断是否节点可以连接上。如果有一半以上的节点去ping一个节点的时候没有回应，集群就认为这个节点宕机了，然后去连接它的备用节点。如果某个节点和所有从节点全部挂掉，我们集群就进入faill状态。还有就是如果有一半以上的主节点宕机，那么我们集群同样进入发力了状态。这就是我们的redis的投票机制，
　　　　(1)投票过程是集群中所有master参与,如果半数以上master节点与master节点通信超时(cluster-node-timeout),认为当前master节点挂掉.
　　　　(2)什么时候整个集群不可用(cluster_state:fail)?
    　　　　a:如果集群任意master挂掉,且当前master没有slave.集群进入fail状态,也可以理解成集群的slot映射[0-16383]不完整时进入fail状态.
    　　　　b:如果集群超过半数以上master挂掉，无论是否有slave，集群进入fail状态.

Redis cluster的slave选举流程：

当slave发现自己的master变为FAIL状态时，便尝试进行Failover，以期成为新的master。由于挂掉的master可能会有多个slave，从而存在多个slave竞争成为master节点的过程， 其过程如下：
1.slave发现自己的master变为FAIL
2.将自己记录的集群currentEpoch加1，并广播FAILOVER_AUTH_REQUEST信息
3.其他节点收到该信息，只有master响应，判断请求者的合法性，并发送FAILOVER_AUTH_ACK，对每一个epoch只发送一次ack
4.尝试failover的slave收集FAILOVER_AUTH_ACK
5.超过半数后变成新Master
6.广播Pong通知其他集群节点。

SLAVE_RANK表示此slave已经从master复制数据的总量的rank。Rank越小代表已复制的数据越新，持有最新数据的slave将会首先发起选举（理论上）

参考：https://www.cnblogs.com/liyasong/p/redis_jiqun.html，https://www.jianshu.com/p/e6894713a6d5

3，redis持久化（rdb和aof）
   RDB：在指定的时间间隔能对数据进行快照存储。
   优点：使用单独子进程来进行持久化，主进程不会进行任何IO操作，保证了redis的高性能
   缺点：RDB是间隔一段时间进行持久化，如果持久化之间redis发生故障，会发生数据丢失，数据的准确性不高。
   AOF：AOF持久化方式记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据,AOF命令以redis协议追加保存每次写的操作到文件末尾.Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大。
   优点：可以保持更高的数据完整性，因此已成为主流的持久化方案
   缺点：AOF文件比RDB文件大，且恢复速度慢。

4，redis高可用原理分析：https://blog.csdn.net/qq_41849945/article/details/80821303。
备注：实际还是一主多从的结构

5，redis如何实现主从复制?以及数据同步机制?
Redis主从复制一般都是异步化完成(复制功能不会阻塞主服务器)，Redis主从复制可以根据是否是全量分为全量同步和增量同步，

<1>Redis全量复制一般发生在Slave初始化阶段，这时Slave需要将Master上的所有数据都复制一份（master生成一份全量的rdb快照文件，发送给slave）。具体步骤如下：
a，从服务器连接主服务器，发送SYNC命令；
b，主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令；
c，主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令；
d，从服务器收到快照文件后丢弃所有旧数据，载入收到的快照；
e，主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令；
f，从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令；

<2>Redis增量复制是指Slave初始化后开始正常工作时主服务器发生的写操作同步到从服务器的过程。
增量复制的过程主要是主服务器每执行一个写命令就会向从服务器发送相同的写命令，从服务器接收并执行收到的写命令。

4，redis如何压缩AOF文件，具体过程如下：
redis调用fork ，现在有父子两个进程
<1>，子进程根据内存中的数据库快照，往临时文件中写入重建数据库状态的命令
<2>，父进程继续处理client请求，除了把写命令写入到原来的aof文件中。同时把收到的写命令缓存起来。这样就能保证如果子进程重写失败的话并不会出问题。
<3>，当子进程把快照内容写入已命令方式写到临时文件中后，子进程发信号通知父进程。然后父进程把缓存的写命令也写入到临时文件。
<4>，现在父进程可以使用临时文件替换老的aof文件，并重命名，后面收到的写命令也开始往新的aof文件中追加。
<5>，需要注意到是重写aof文件的操作，并没有读取旧的aof文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的aof文件,这点和快照有点类似。

简单总结：如何缩小AOF文件大小：文件重写是指定期重写AOF文件（产生新的AOF文件），减小AOF文件的体积。需要注意的是，AOF重写是把Redis进程内的数据转化为写命令，同步到新的AOF文件（为了压缩aof的持久化文件。redis提供了bgrewriteaof命令。收到此命令redis将使用与快照类似的方式将内存中的数据 以命令的方式保存到临时文件中，最后替换原来的文件）

参考：http://www.cnblogs.com/xingzc/p/5988080.html

5，AOF缩减自身文件大小的时候，来了新的写请求怎么办？
  子进程同步完内存中数据之后，会发出指令，通知父进程把最近的写请求操作刷入新的aof文件。

6，redis multi，pipeline的区别
redis交互流程：业务应用服务器（如hotel-goods-service）--->redis client--->redis server

备注：redis属于典型的c/s架构

multi特点：
<1>实际上当我们使用multi操作时，redis client是一条条发送数据到 redis server，这些请求是积压在服务端的queue里面，然后依次一次执行完毕，redis服务端一次性返回所有命令返回结果，服务端执行这段操作是开启事务机制的。
<2>由于每发送一条指令，都需要单独发给服务器，服务器再单独返回“该条指令已加入队列”这个消息。这是比Pipeline慢的原因之一。
<3>Multi执行的时候会先暂停其他命令的执行(事务机制)，类似于加了个锁，直到整个Multi结束完成再继续其他客户端的请求。这是Multi能保证一致性的原因，也是比Pipeline慢的原因之二
<4>由于服务端开启了事务机制，因此multi是原子性的。
<5>由于redis client逐条发送请求到redis server中的queue，因此multi属于服务端缓冲。

pipeline特点：
<1>redis client将所有命令打包一次性发送。发送成功后，服务端不用返回类似“命令已收到”这样的消息，而是一次性批量执行所有命令，成功后再一次性返回所有处理结果。
<2>服务端处理命令的时候，不需要加锁，而是与其他客户端的命令混合在一起处理，所以无法保证一致性。
<3>由于是redis client一次打包发送出去的请求，因此pipeline是客户端缓冲
<4>由于pipeline把请求打包发送给redis server，少了与redis server的多次交互，因此性能更加好。

参考：https://blog.walkerx.cn/2018/07/08/redis-multi-pipeline/，https://www.cnblogs.com/harryc/p/6005165.html，https://blog.csdn.net/u011489043/article/details/78769428

8，redis动态扩容
过程描述：
HASH_SLOT = CRC16(key) mod 16384
通过key与slot的映射算法，计算出当前key应该存储在哪个slot中，从公式中可以看出，当前key与slot的映射是固定不变的。由于每个Master负责一部分slot，可知在Master节点数量调整时，slot与Master映射的关系也会调整，也就是说slot和master之间有个映射表的。

在动态扩容过程中slot的特点：
举例：MasterA节点（原集群中的旧机器）迁移部分slot到MasterB节点
MIGRATING状态是发生在MasterA节点中的一种槽的状态，预备迁移槽的时候槽的状态首先会变为MIGRATING状态，这种状态的槽会实际产生什么影响呢?当客户端请求的某个Key所属的槽处于MIGRATING状态的时候，影响有下面几条

<1>如果Key存在则成功处理
<2>如果Key不存在，则返回客户端ASK，仅当这次请求会转向另一个节点，并不会刷新客户端（redis-client）中node的映射关系，也就是说下次该客户端请求该Key的时候，还会选择MasterA节点
<3>如果Key包含多个命令，如果都存在则成功处理，如果都不存在，则返回客户端ASK，如果一部分存在，则返回客户端TRYAGAIN，通知客户端稍后重试，这样当所有的Key都迁移完毕的时候客户端重试请求的时候回得到ASK，然后经过一次重定向就可以获取这批键。

IMPORTING状态是发生在MasterB节点中的一种槽的状态，预备将槽从MasterA节点迁移到MasterB节点的时候，槽的状态会首先变为IMPORTING。IMPORTING状态的槽对客户端的行为有下面一些影响：

<1>正常命令会被MOVED重定向，如果是ASKING命令则命令会被执行，从而Key没有在老的节点已经被迁移到新的节点的情况可以被顺利处理；
<2>如果Key不存在则新建；
<3>没有ASKING的请求和正常请求一样被MOVED，这保证客户端node映射关系出错的情况下不会发生写错；

简述：redis在动态扩容时，需要集群里面的旧机器的部分slot迁移到新机器，由于只涉及部分slot迁移，因此这些待迁移的slot会变成迁移状态（MIGRATING），迁移过程中旧机器仍然接受读取和写入流量，如果key在旧机器不存在，请求将转发到新扩容的节点（如MasterB），等数据迁移完成再更新slot映射关系表即可。

参考：https://www.cnblogs.com/wxd0108/p/5729754.html，https://blog.csdn.net/men_wen/article/details/72896682

9，单机redis如何提高并发
<1>redis性能瓶颈在io，因此单key不应该存储大值。
<2>pipeline代替multiGet操作。
<3>写入redis的数据做压缩。
<4>当业务场景不需要数据持久化时，关闭所有的持久化方式可以获得最佳的性能(数据持久化时需要在持久化和延迟/性能之间做相应的权衡，实际上是在持久化的时候，数据占用了内核的页缓存，导致可用页缓存空间紧张)
<5>存储对象使用hash，避免修改其中一项信息时，需要把整个对象取回，并且修改操作需要对并发进行保护。

参考：https://www.cnblogs.com/moonandstar08/p/7282108.html

10，redis内存优化
Redis 最为常用的数据类型主要有以下五种：String，Hash，List，Set，Sorted set
Redis任何的数据类型都是由一个叫做redisObject的数据结构管理的，具体参考如下
----------------------
   数据类型（type）     |
----------------------
   编码方式（encoding） |
----------------------
   数据指针（ptr）	     |
----------------------
   虚拟内存（vm）       |
----------------------
   LRU计时时钟         |
----------------------

数据类型：string，list，hash，set，sorted set，
编码方式：raw，int，ht，zipmap，linkedlist，ziplist，intset
lru计时时间：记录对象最后一次被访问的时间，当配置了 maxmemory和maxmemory-policy=volatile-lru | allkeys-lru 时， 用于辅助LRU算法删除键数据。可以使用object idletime {key}命令在不更新lru字段情况下查看当前键的空闲时间

备注：首先最重要的一点是不要开启 Redis 的 VM 选项，即虚拟内存功能，这个本来是作为 Redis 存储超出物理内存数据的一种数据在内存与磁盘换入换出的一个持久化策略，但是其内存管理成本也非常的高，并且我们后续会分析此种持久化策略并不成熟，所以要关闭 VM 功能，请检查你的 redis.conf 文件中 vm-enabled 为 no。 其次最好设置下redis.conf中的 maxmemory 选项，该选项是告诉 Redis 当使用了多少物理内存后就开始拒绝后续的写入请求，该参数能很好的保护好你的 Redis 不会因为使用了过多的物理内存而导致 swap，最终严重影响性能甚至崩溃

Redis内部使用一个redisObject对象来表示所有的key和value,redisObject最主要的信息如上图所示：type代表一个value对象具体是何种数据类型，encoding是不同数据类型在redis内部的存储方式，比如：type=string代表value存储的是一个普通字符串，那么对应的encoding可以是raw或者是int,如果是int则代表实际redis内部是按数值型类存储和表示这个字符串的，当然前提是这个字符串本身可以用数值表示，比如:"123" "456"这样的字符串，当使用int存储时，比使用raw存储原生的字符串更加节省内存。

Redis内存优化点：
<1>存储字符串时，如果值是整数，内部转成int存储，节省空间。
<2>Redis Hash是value内部为一个 HashMap，如果该Map的成员数比较少，则会采用类似一维线性的紧凑格式来存储该Map即省去了大量指针的内存开销，具体配置参数如下：
hash-max-zipmap-entries 64
hash-max-zipmap-value 512
含义是当 value 这个 Map 内部不超过多少个成员时会采用线性紧凑格式存储，默认是64，即 value 内部有64个以下的成员就是使用线性紧凑存储，超过该值自动转成真正的 HashMap， hash-max-zipmap-value 含义是当 value 这个 Map 内部的每个成员值长度不超过多少字节就会采用线性紧凑存储来节省空间。以上2个条件任意一个条件超过设置值都会转换成真正的 HashMap，也就不会再节省内存了，那么这个值是不是设置的越大越好呢，答案当然是否定的，HashMap 的优势就是查找和操作的时间复杂度都是 O(1) 的，而放弃 Hash 采用一维存储则是 O(n) 的时间复杂度，如果成员数量很少，则影响不大，否则会严重影响性能，所以要权衡好这个值的设置，总体上还是最根本的时间成本和空间成本上的权衡。
<3>共享对象池，对象共享池指Redis内部维护[0-9999]的整数对象池。创建大量的整数类型redisObject存在内存开销，每个redisObject内部结构至少占16字节，甚至超过了整数自身空间消耗。所以Redis内存维护一个[0-9999]的整数对象池，用于节约内存。 除了整数值对象，其他类型如list,hash,set,zset内部元素也可以使用整数对象池。因此开发中在满足需求的前提下，尽量使用整数对象以节省内存。


参考：https://www.cnblogs.com/jandison/p/6902396.html，https://www.cnblogs.com/_popc/p/5968683.html

11，页缓存技术
Page cache是通过将磁盘中的数据缓存到内存中，从而减少磁盘I/O操作，从而提高性能。此外，还要确保在page cache中的数据更改时能够被同步到磁盘上，后者被称为page回写（page writeback）。一个inode对应一个page cache对象，一个page cache对象包含多个物理page。
对磁盘的数据进行缓存从而提高性能主要是基于两个因素：第一，磁盘访问的速度比内存慢好几个数量级（毫秒和纳秒的差距）。第二是被访问过的数据，有很大概率会被再次访问。
参考：https://blog.csdn.net/damontive/article/details/80552566

12，redis的过期数据删除策略和内存淘汰策略

一、过期数据删除策略（redis是两种策略配合一块使用）
<1>惰性删除，不管过期的键，在这种策略下，当键在键空间中被取出时，首先检查取出的键是否过期，若过期删除该键，否则，返回该键。很明显，惰性删除依赖过期键的被动访问，对于内存不友好，如果一些键长期没有被访问，会造成内存泄露（垃圾数据占用内存），但是它属于cpu友好型，不需要占用太多cpu时间片。
<2>定期删除，redis创建一个定时任务随机扫描数据是否过期（CPU空闲时在定期serverCron任务中），逐出部分过期Key，具体删除过程如下

A,Redis配置项hz定义了serverCron任务的执行周期，默认为10，即CPU空闲时每秒执行10次;
B,每次过期key清理的时间不超过CPU时间的25%，即若hz=1，则一次清理时间最大为250ms，若hz=10，则一次清理时间最大为25ms;
C,清理时依次遍历所有的db;
D,从db中随机取20个key，判断是否过期，若过期，则逐出;
E,若有5个以上key过期，则重复步骤4，否则遍历下一个db;
F,在清理过程中，若达到了25%CPU时间，退出清理过程;

备注：每次删除限定在25%cpu时间片范围内，并且还判断过期的key的比例，比如超过25%过期key才继续下一此删除。这是一个基于概率的简单算法，基本的假设是抽出的样本能够代表整个key空间，redis持续清理过期的数据直至将要过期的key的百分比降到了25%以下。这也意味着在长期来看任何给定的时刻已经过期但仍占据着内存空间的key的量最多为每秒的写操作量除以4。

二、内存淘汰策略
Redis内存淘汰策略被激发，是内存使用达到一定的阈值才开始运行的（redis.conf里面配置），因此内存淘汰策略和key过期删除策略是两码事。

noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。
allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。
allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。
volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。
volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。
volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。
以上策略是：（非）过期、随机、lru的组合

redis过期字典：
redisDb结构的expires字典保存了数据库中所有键的过期时间，为过期字典，键就是数据库键，值是long long类型，毫秒经度的unix时间戳（过期时间）。


举例简述redis 3.0对allkeys-lru的实现流程
Redis服务器每执行一个命令，都会检测内存，判断是否需要进行数据淘汰
<1>判断目前已经使用的内存大小是否比设置的maxmemory要小，如果小于maxmemory，那么无须执行进一步操作。
<2>判断淘汰策略是否为noeviction，如果是，直接return回去，不进行任何内存淘汰。
<3>根据传入的对象大小，计算需要释放多少字节的内存
<4>开始随机采样，每次随机获取10个key，选出lru时间最小的key放入一个长度为16的pool里面，后续再不断随机取样10个key，如果lru时间比pool最小lru时间还小，就加入pool，直至pool填满，然后开始淘汰pool里面lru时间最小的，直至淘汰的空间足够存储需要的值。

备注：Redis的内存淘汰策略的选取并不会影响过期的key的处理。内存淘汰策略用于处理内存不足时的需要申请额外空间的数据；过期策略用于处理过期的缓存数据。
redis的 lru算法实际上不是非常准确的，是基于快速抽样比较的实现（如果使用双向链表的指针标记，占用的空间更加大）
参考：https://yq.aliyun.com/articles/257459，https://www.jianshu.com/p/d45424d2c610（内存淘汰策略源码），https://segmentfault.com/a/1190000017555834

13，redis集群访问流量倾斜
<1>hot key出现造成集群访问量倾斜
场景：Hot key，即热点 key，指的是在一段时间内，该 key 的访问量远远高于其他的 redis key， 导致大部分的访问流量在经过 proxy 分片之后，都集中访问到某一个 redis 实例上。hot key 通常在不同业务中，存储着不同的热点信息。比如：新闻应用中的热点新闻内容，活动系统中某个用户疯狂参与的活动的活动配置，商城秒杀系统中，最吸引用户眼球，性价比最高的商品信息。
解决方案：
一，对hot key数据进行本地缓存。
二，利用分片算法的特性，对key进行打散处理，比如对key加上0~9的后缀，redis key经过分片分布到不同的实例上，将访问量均摊到所有实例。
<2>big key在redis存储优化
如果big value 是个大json 通过 mset 的方式，将这个key的内容打散到各个实例中（分段存储），减小big key对数据量倾斜造成的影响。


7，mybatis延迟加载。
   resultMap可以实现高级映射(使用association、collection实现一对一及一对多映射)，association、collection具备延迟加载功能。
　　延迟加载：先从主表查询，需要时再从关联表去关联查询，大大提高数据库性能，因为查询单表要比关联查询多张表速度要快。
   场景举例：查询出符合要求的订单数据，再去查出这些订单的用户信息，整个过程要执行多个sql，但是在一个resultMap返回结果。

实现原理：在createResultObject的时候，会判断当前返回值是否含有延迟加载的数据，如果有，就创建动态代理对象（Javasisst或者Cglib代理），执行被代理的方法，获取数据，并封装到resultMap。
延迟加载的好处：先在单表查询、需要时再从关联表去关联查询，大大提高 数据库性能，因为查询单表要比关联查询多张表速度要快。
参考：https://my.oschina.net/wenjinglian/blog/1857581，https://blog.csdn.net/qq924862077/article/details/53997740

9，mybatis中#{}和${}的区别
  #将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号（很大程度上防止了sql注入），$将传入的数据直接显示生成在sql中（无法防止sql注入）。


10，tcp三次握手（两次不行吗？），四次挥手，为什么这么做。
三次握手是为了建立tcp的双工通信，四次挥手是为了能够保证tcp的半闭合状态。

11，网络丢包如何解决，分不同业务场景。（ack，滑动窗口）

9，读写锁源码（todo）
参考：https://blog.csdn.net/yanyan19880509/article/details/52435135
10，各种线程实现。

         //线程池大小固定为1
         Executors.newSingleThreadExecutor();

        //固定大小线程池由自己设定，即自己控制资源的固定分配
        Executors.newFixedThreadPool(10);

        //动态调整线程池的大小，最小为0，最大为int最大值，，newCachedThreadPool会大幅度提高大量短暂异步任务的性能，
        //如果执行业务逻辑比较慢，会导致持续创建线程，导致cpu资源消耗殆尽
        //为什么使用SynchronousQueue？最多只能持有一个任务数据，当任务数据插入队列失败，会驱动创建新线程，SynchronousQueue作为主线程池的工作队列，它是一个没有容量的阻塞队列。每个插入操作必须等待另一个线程的对应移除操作。这意味着，如果主线程提交任务的速度高于线程池中处理任务的速度时，CachedThreadPool会不断创建新线程。极端情况下，CachedThreadPool会因为创建过多线程而耗尽CPU资源
        Executors.newCachedThreadPool();//newCachedThreadPool不适合io密集型的网络请求，只适合计算密集型（每次请求耗时很短）

        //基于延迟队列实现的延时任务线程池，周期性的执行所提交的任务
        ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(10);
        scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
            @Override
            public void run() {
                System.out.println(Thread.currentThread().getName() + " run");
            }
        }, 1000,2000, TimeUnit.MILLISECONDS);



11，各种队列实现。
<1>PriorityBlockingQueue（无界队列） 内部使用reentrantlock（基于数组实现的堆排序，数组会动态扩容），每次入队和出队都需要加锁，保证线程安全。
   PriorityBlockingQueue存储的对象必须是实现Comparable接口的 因为PriorityBlockingQueue队列会根据内部存储的每一个元素的compareTo方法比较每个元素的大小
   需要注意的是PriorityBlockingQueue并不会阻塞数据生产者，而只会在没有可消费的数据时，阻塞数据的消费者。因此使用的时候要特别注意，生产者生产数据的速度绝对不能快于消费者消费数据的速度，否则时间一长，会最终耗尽所有的可用堆内存空间。在实现PriorityBlockingQueue时，内部控制线程同步的锁采用的是公平锁。PriorityBlockingQueue在take出来的时候会根据优先级 将优先级最小的最先取出

备注：优先队列扩容阶段为什么释放锁，因为只有一把锁，扩容期间不影响数据读取（提高并发效率），扩容完之后再拷贝以前的数据（拷贝阶段加锁就可以了）。
由于PriorityBlockingQueue在空间不够的时候，会自增扩容数组进行堆排序，不需要对数据的put操作进行阻塞，只对数据获取进行阻塞，因此只需要一个condition（唤醒数据查询的线程）

<2>ArrayBlockingQueue，基于数组实现，只有1个锁（数据的写入不需要构造node节点，直接存储外部传入的引用，效率已经足够高，LinkedBlockingQueue构造node节点，耗时相对高一些，因此读写锁分离为了提高并发效率），添加数据和删除数据的时候只能有1个被执行，不允许并行执行。使用Condition notEmpty，Condition notFull来实现生产者-消费者模式(通知模式)。

<3>LinkedBlockingQueue，基于链表实现，只有2个锁（由于是无界，因此不用担心队列写满，读写可以分离），放锁和读锁，两把锁分别管理head节点和last节点的操作，通过原子变量count控制队列长度状态，添加数据和删除数据是可以并行进行的，当然添加数据和删除数据的时候只能有1个线程各自执行。LinkedBlockingQueue将读和写操作分离，可以让读写操作在不干扰对方的情况下，完成各自的功能，提高并发吞吐量。使用Condition notEmpty，Condition notFull来实现生产者-消费者模式(通知模式)

备注：ArrayBlockingQueue和LinkedBlockingQueue这两个阻塞队列，队列满了，放不进去会被阻塞，队列为空，取不出结果会被阻塞，因此需要两个condition。
压测报告：一千万条的数据进行多线程插入和读取，明显看出ArrayBlockingQueue比LinkedBlockingQueue性能强30%。

<4>
SynchronousQueue通过将入队出队的线程绑定到队列的节点上，并借助LockSupport的park()和unpark()实现等待和唤醒，先到达的线程A需调用LockSupport的park()方法将当前线程进入阻塞状态，知道另一个与之匹配的线程B调用LockSupport.unpark(Thread)来唤醒在该节点上等待的线程A。其内部没有任何容量，任何的入队操作都需要等待其他线程的出队操作，反之亦然。如果将SynchronousQueue用于生产者/消费者模式，那么相当于生产者和消费者手递手交易，即生产者生产出一个货物，则必须等到消费者过来取货，方可完成交易。

备注：SynchronousQueue没有使用condition（本质上基于LockSupport实现），直接使用了LockSupport的park()和unpark()实现等待和唤醒
参考：https://blog.csdn.net/vickyway/article/details/50113429

<5>
DelayQueue的泛型参数需要实现Delayed接口，Delayed接口继承了Comparable接口，DelayQueue内部使用非线程安全的优先队列（PriorityQueue），并使用Leader/Followers模式，最小化不必要的等待时间。DelayQueue不允许包含null元素。（借助LockSupport.parkNanos和unpark实现延时，reentrantlock实现安全操作），available.awaitNanos(delay)实现延时，available.awaitNanos内部基于LockSupport.parkNanos(this, nanosTimeout)实现挂起，指定时间范围内自动唤醒（由操作系统自己去调度）;

特点：元素进入队列后，先进行排序(调用compareTo方法排序)，然后，只有getDelay也就是剩余时间为0的时候，
该元素才有资格被消费者从队列中取出来，实际上只有队列头元素出队，其它才能出队，会受到头结点元素延时时间的影响。
备注：DelayQueue使用PriorityQueue（自动扩容的堆数组），因此数据的存入不需要阻塞，读取的时候才进行堵塞，因此只需要一个condition。



13，线程池核心线程如何设置。
14，Spring如何处理循环引用的
   <1>，循环依赖的对象都通过构造器注入，会注入失败。（无论bean是singleton，还是prototype，或者是混合了singleton、prototype），因为生成对象必须依赖构造方法，而构造方法里面需要对方的实例对象，因此形成了死循环。
   <2>，循环依赖的bean都是通过属性注入，如果注入都是singleton对象，都能创建成功。如果注入都是prototype，就会失败。如果是混合singleton、prototype，只有先创建singleton才能保证成功，否则就会失败。
Spring对于构造器注入的对象会标记为正在创建中，如果在循环依赖创建过程中发生依赖的对象正在创建中，会抛出异常。
   归根结底是spring容器内部保留了singleton对象，prototype对象被丢失。
备注：singleton对象有三级缓存的概念，prototype对象没有，singleton在创建过程中会检查三级缓存，依次从里面取出数据（前提这些对象都是调用构造方法创建成功了），如果成功取出就完成注入。prototype对象在创建过程中会被标记为“正在创建中”，如果循环创建中，发现依赖的bean处于“正在创建中”，就会抛出异常。

   参考：https://blog.csdn.net/chen2526264/article/details/80673598，https://blog.csdn.net/chejinqiang/article/details/80003868（bean创建的三级缓存），https://blog.csdn.net/u010853261/article/details/77940767

15，spring初始化对象

单例对象（基于三级缓存实现）
（1）createBeanInstance：实例化，其实也就是调用对象的构造方法实例化对象
（2）populateBean：填充属性，这一步主要是多bean的依赖属性进行填充
（3）initializeBean：调用spring xml中的init 方法。
 单例来说，在Spring容器整个生命周期内，有且只有一个对象，所以很容易想到这个对象应该存在Cache中，Spring为了解决单例的循环依赖问题，使用了三级缓存。

prototype对象（完成初始化之前存在一个set里面）
初始化流程，在初始化属性的时候，isPrototypeCurrentlyInCreation，会校验这个属性的bean是否在创建中，如果在创建中会抛出异常。

16，spring ioc
ioc，依赖注入，在以前的软件工程编码过程中，类的属性需要硬编码生成对象数据，耦合性较高，如果使用ioc，是在容器启动过程中，在bean对象实例化过程中需要检查其依赖数据，并且进行数据注入（setter，构造器注入），完成一个对象的实例化并实现了解耦合，并且能够对这些对象进行复用。

aop，主要分为两大类：一是采用jdk动态代理技术，利用截取消息的方式，对该消息进行装饰，以取代原有对象行为的执行；二是采用动态织入的方式，引入特定的语法创建“方面”，是在类加载时期织入有关“方面”的代码。它利用一种称为"横切"的技术，并将那些影响了多个类的公共行为封装到一个可重用模块，简单理解是抽象出与业务逻辑无关的公共行为逻辑。

17，java的future编程
FutureTask实现了Runnable, Future接口，并实例化Callable对象，在线程开启运行时，执行线程任务的实现类的run方法，run执行完毕将结果引用赋值给outcome属性（如果任务线程没执行完，当前主线程会进入阻塞状态，任务线程执行完毕，会设置outcome，并解除主线程阻塞）。

18，hystrix

<1> 使用场景：在soa架构中，资源隔离（线程池、或者信号量隔离），熔断（防止雪崩效应）降级，依赖的服务使用不同的commandKey（最小隔离单元，可能多个commandKey在一个线程池内）标注，实现隔离，线程池是HystrixCommandGroupKey标识。
<2> hystrix是如何通过线程池实现线程隔离的
Hystrix通过命令模式，将每个类型的业务请求封装成对应的命令请求，比如查询订单->订单Command，查询商品->商品Command，查询用户->用户Command。每个类型的Command对应一个线程池。创建好的线程池是被放入到ConcurrentHashMap中，比如查询订单。
<3> hystrix如何实现熔断的
用户请求某一服务之后，Hystrix会先经过熔断器，此时如果熔断器的状态是打开，则说明已经熔断，这时将直接进行降级处理，不会继续将请求发到线程池。如果熔断器是关闭状态，会检测最近10秒的请求错误率，当错误率超过预设的值（默认是50%）且10秒内超过20个请求，则开启熔断。熔断器默认是在5s后开始重新嗅探，会尝试放过去一部分流量进行试探，确定依赖服务是否恢复。

<4> hystrix如何统计失败率
每个熔断器默认维护10个bucket
每秒创建一个bucket
每个blucket记录成功,失败,超时,拒绝的次数
当有新的bucket被创建时，最旧的bucket会被抛弃

<5> 核心参数
HystrixCommandGroupKey，线程池分组
HystrixCommandKey，线程池标识。

Circuit Breaker（熔断器）一共包括如下6个参数。
1、circuitBreaker.enabled
是否启用熔断器，默认是TURE。
2、circuitBreaker.forceOpen
熔断器强制打开，始终保持打开状态。默认值FLASE。
3、circuitBreaker.forceClosed
熔断器强制关闭，始终保持关闭状态。默认值FLASE。
4、circuitBreaker.errorThresholdPercentage
设定错误百分比，默认值50%，例如一段时间（10s）内有100个请求，其中有55个超时或者异常返回了，那么这段时间内的错误百分比是55%，大于了默认值50%，这种情况下触发熔断器-打开。
5、circuitBreaker.requestVolumeThreshold
默认值20.意思是至少有20个请求才进行errorThresholdPercentage错误百分比计算。比如一段时间（10s）内有19个请求全部失败了。错误百分比是100%，但熔断器不会打开，因为requestVolumeThreshold的值是20. 这个参数非常重要，熔断器是否打开首先要满足这个条件。
6、circuitBreaker.sleepWindowInMilliseconds
半开试探休眠时间，默认值5000ms。当熔断器开启一段时间之后比如5000ms，会尝试放过去一部分流量进行试探，确定依赖服务是否恢复

参考：http://itindex.net/detail/57782-hystrix-%E7%86%94%E6%96%AD%E5%99%A8-%E6%8A%80%E6%9C%AF

16，CMS垃圾收集器与G1收集器



0，基本数据类型：

byte：Java中最小的数据类型，在内存中占8位(bit)，即1个字节，取值范围-128~127，默认值0
short：短整型，在内存中占16位，即2个字节，取值范围-32768~32717，默认值0
int：整型，用于存储整数，在内在中占32位，即4个字节，取值范围-2147483648~2147483647，默认值0
long：长整型，在内存中占64位，即8个字节-2^63~2^63-1，默认值0L
float：浮点型，在内存中占32位，即4个字节，用于存储带小数点的数字（与double的区别在于float类型有效小数点只有6~7位），默认值0
double：双精度浮点型，用于存储带有小数点的数字，在内存中占64位，即8个字节，默认值0
char：字符型，用于存储单个字符，占16位，即2个字节，取值范围0~65535，默认值为空
boolean：布尔类型，占1个字节，用于判断真或假（仅有两个值，即true、false），默认值false

1，Java的引用类型：
强引用、弱引用、软引用、虚引用
1，强引用是使用最普遍的引用。如果一个对象具有强引用，那垃圾回收器绝不会回收它。
2，如果一个对象只具有软引用，则内存空间足够，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。
3，弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。
4， “虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。
    虚引用主要用来跟踪对象被垃圾回收器回收的活动。虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之 关联的引用队列中

1，WeakReference如字面意思，弱引用， 当一个对象仅仅被weak reference（弱引用）指向, 而没有任何其他strong reference（强引用）指向的时候, 如果这时GC运行, 那么这个对象就会被回收，不论当前的内存空间是否足够，这个对象都会被回收
2，如果一个对象只具有软引用(SoftReference)，则内存空间足够，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存

3，ThreadLocal什么时候出现内存泄漏？ThreadLocal里面为啥使用了WeakReference？
Thread实例为每个ThreadLocal对象维护了一个副本，这个副本数据存放在ThreadLocalMap里面，因此才做到线程间的数据不共享。
<1>当一个ThreadLocal实例被直接赋值为null（没有调用set，remove），此时会出现内存泄漏，因为thread实例里面的ThreadLocalMap保存了ThreadLocal的引用，假设此时线程没有被销毁，因此在gc的时候并不能回收这部分空间，就是说出现了内存泄漏（ThreadLocal直接赋值为null的方式，无论使用强弱引用都无法解决内存泄漏的问题）。
<2>如果使用弱引用（实际是ThreadLocalMap的Entry类的key才使用弱引用，value没有使用，ThreadLocalMap里面放就是Entry弱引用，其封装了ThreadLocal），在ThreadLocal对象被赋值为null，会导致弱引用在gc的时候，Entry的key被回收并变成null，使用弱引用可以多一层保障：对应的value在下一次ThreadLocalMap调用set,get,remove的时候会被清除（这些方法的内部对Entry的key为null的value数据进行清除）。

备注：ThreadLocal是一个类，当实例化一个ThreadLocal对象时，会在当前线程Thread创建一个ThreadLocalMap，这个ThreadLocalMap里面存放了Entry，Entry是由ThreadLocal（key）和value（实际的数据）构成。Entry的key是通过弱引用封装，如果ThreadLocal没有外部指向（即被赋值为null）时，那Entry的key在gc的时候就会被回收，当此线程的ThreadLocalMap被再次访问时，会自动删除以前Entry的key为null的value数据。

参考：https://blog.csdn.net/wudiyong22/article/details/52141608


4，内存溢出和内存泄漏的区别
   内存溢出（Out Of Memory，OOM），就是内存不够用了，内存泄漏（Memory Leak），指的是申请的内存空间，自己没有去主动释放，gc也无法释放（如强引用），多次内存泄漏，就会导致内存溢
   memory leak会最终会导致out of memory！

2，Arraylist初始容量为10，每次扩容1.5倍，原来元素拷贝过去，hashMap初始化容量是16，负载因子0.75，每次容量达到（0.75*上次容量）开始扩容2倍

3,线程池核心线程大小设置，机器内核数量，qps，相应时间关系
<1>如果是计算密集型的服务，由于cpu处理效率非常高，核心线程一般设置为内核N+1（1为等待cpu的时间片）
<2>如果是io耗时较高的服务，一般设置为（qps*99线）/1000，其中99线为毫秒

4，简述线程池的实现：线程池把每个提交到线程池的任务封装成一个worker（并且实现了Runnable接口），当第一批任务到达的时候（corePool还没到最大值），此时new出的线程开始执行任务，执行完，并且去消费队列，如果coreSize满了，此时队列就有值了，这时候就会消费队列里面的任务了，实际上是利用阻塞队列的take方法维持核心线程的存活，如果队列满了，就会创建新线程，直至达到maxSizePool，在消费队列中的任务数据的同时，如果线程在keepAlive时间范围内获取不到队列数据，就会释放最大线程，是通过workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS)控制非核心线程的存活，如果从队列获取不到数据，就从worker集合删除该线程。

5，信号量的使用：把信号量比作资源，允许多线程去使用资源，但是只能允许部分线程使用。semaphore构造方法初始化资源大小，semaphore.acquire()获取资源，semaphore.release()释放资源。
  CountDownLatch和Semaphore底层实现都是基于AbstractQueuedSynchronizer，CountDownLatch和Semaphore属于典型的共享锁。CyclicBarrier用来给多个线程之间进行互相等待其他所有线程而设计的（并且实现了可重用机制，每次计数到0，自动重新设置为起始值），而CountDownLatch是给一个起"调度"其它一组线程用的,这个线程关键职责就是等待进行其他工作线程返回。

备注：CountDownLatch.await阻塞主线程，CountDownLatch.countDown计数变量递减，递减到0会唤醒主线程，cyclicBarrier.await()当做栅栏，让一组线程都在栅栏之前完成任务，内部会做计数，只有变成0，才能让所有线程复活。
 CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier。

6，在很多情况下，可能有多个线程需要访问数目很少的资源。假想在服务器上运行着若干个回答客户端请求的线程。这些线程需要连接到同一数据库，但任一时刻只能获得一定数目的数据库连接。你要怎样才能够有效地将这些固定数目的数据库连接分配给大量的线程？
解决方案：比如把资源放到阻塞队列，或者放到信号量里面。

7，ConcurrentHashMap的原理
1.6和1.7的实现：分为了16个segement，每个segement内部再次实现一次hashmap，因此查找一个数据需要两次hash（先找segement，再找segement里面的hash位置），put操作是在segement维度使用了reentrantlock，get是先找到segement，再查找数据，找到就对整个segement加锁。size方法是比较两次结果，如果不相等，就对每个segement加锁，重新计算（为什么要比较两次？在两次比较的过程中，被修改的概率很小，如果要加锁，就会导致整个map锁大量竞争（读多写少的时候），不如一开始不用锁的方式进行比较）。
1.8：不再使用segement，直接使用node数组+链表，当链表长度达到8，会升级为红黑树。put操作是使用了synchronize对当前链表加锁，get是使用Unsafe.getObjectVolatile获取最新的共享内存值（不加锁）。

9，Object的notify 和 notifyAll的区别
notify方法只唤醒一个等待（对象的）线程并使该线程开始执行。所以如果有多个线程等待一个对象，这个方法只会唤醒其中一个线程，选择哪个线程取决于操作系统对多线程管理的实现。notifyAll 会唤醒所有等待(对象的)线程，尽管哪一个线程将会第一个处理取决于操作系统的实现。如果当前情况下有多个线程需要被唤醒，推荐使用notifyAll 方法。比如在生产者-消费者里面的使用，每次都需要唤醒所有的消费者或是生产者，以判断程序是否可以继续往下执行。

notify只是唤醒正在等待的线程，至于什么时候开始竞争，取决于当前线程什么时候释放。（ReentrantLock对应的condition.signal方法也是如此）

10，可重入锁（ReentrantLock）的使用场景：当前线程内部逻辑进行递归调用

11，synchronized（独占锁），多线程使用，结合object的wait、notify（notifyAll）使用时注意的问题，调用wait，是释放当前线程持有某个对象的锁，让给其它线程竞争，并且由它们通知回调。
备注：使用wait、notify（notifyAll）方法前提必须是当前线程持有锁，也就是说必须在synchronized模块内使用
     synchronized的锁标记存放在Java对象头的Mark Word中，同步代码块采用monitorenter、monitorexit指令（c++层面）显式的实现。

12，ReentrantLock（独占锁），多线程使用，结合Condition（condition = myLock.newCondition()），condition.await()和signal、signalAll()通知其它线程进行锁的竞争。
备注：1，使用await、signal（signalAll）方法前提必须是当前线程持有锁（也就是说一个线程不能释放别的线程持有的锁）
     2，reentrantlock的lock方法如果获取不到锁，会被阻塞，tryLock获取不到，立刻返回false，tryLock(long time, TimeUnit unit)是对获取锁加上时间控制
     3，condition.await()，将一个线程的锁交出，当前线程进入挂起状态（cpu时间片交出），当前线程放入等待锁的双向队列（AQS）里面，这个线程同时也被另外一个condition队列维护，condition.signal()调用
时，将双向队列中的线程设置为可抢锁状态，condition队列的头结点删除此线程数据。
     4，condition.await()，是由用户的其它线程唤醒，condition.await(time)，这是由内核在指定时间后去帮你唤醒的

13，静态代理和动态代理区别
    静态不够灵活，需要针对每个被代理类的接口都对应开发一个代理类的接口，代码维护成本比较高。
14，动态代理实现的两种方式和区别
   java动态代理是利用反射机制生成一个实现代理接口的匿名类，在调用具体方法前调用InvokeHandler来处理。
   cglib动态代理是利用asm开源包，对代理对象类的class文件加载进来，通过修改其字节码生成子类来处理

　　JDK动态代理只能对实现了接口的类生成代理，而不能针对类
　　CGLIB是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法（继承）

15，CGlib比JDK代理快？

　 (1)使用CGLib实现动态代理，CGLib底层采用ASM字节码生成框架，使用字节码技术生成代理类，比使用Java反射效率要高。唯一需要注意的是，CGLib不能对声明为final的方法进行代理，因为CGLib原理是动态生成被代理类  的子类。
　 (2)在对JDK动态代理与CGlib动态代理的代码实验中看，1W次执行下，JDK7及8的动态代理性能比CGlib要好20%左右。

16，Java 序列化做了哪些事情
Java的序列化算法一般会按步骤做如下事情：
◆将对象实例相关的类元数据输出。
◆递归地输出类的超类描述直到不再有超类。
◆类元数据完了以后，开始从最顶层的超类开始输出对象实例的实际数据值。
◆从上至下递归输出实例的数据

17，简述公平锁和非公平锁的实现。
    Reentrantlock支持公平和非公平模式，实现锁的最基础组件类是：内部类NonfairSync和FairSync，外部AbstractQueuedSynchronizer（抽象队列同步器，AQS），公平锁和非公平锁在获取锁时都尝试性去获取，当获取失败才进入有序等待队列中（先进先出的双向链表），并且这些线程会被挂起（让出cpu时间片），公平锁在获取锁时，会判断当前线程是否是队列头结点线程（hasQueuedPredecessors），如果是头结点才有权拿到锁。非公平锁在获取锁时，是没在队列中的线程和队列的头结点竞争（即获取锁时，不对线程是否是头结点线程做限制）。当一个锁被释放时，它会去唤醒等待队列中的头结点，因此才出现新来线程和头结点竞争。

简单理解：公平是按顺序加锁，非公平是不保证按顺序加锁（实际上是外部线程和队列中线程竞争），处于阻塞状态的线程必须依赖别的线程释放锁，才能被唤醒去获取锁。

参考：https://cloud.tencent.com/developer/article/1371124，https://www.cnblogs.com/gym333/p/6343971.html，https://www.jianshu.com/p/cc308d82cc71

18，AbstractQueuedSynchronizer为什么使用双向队列？
aqs为什么使用双向队列（即双向链表）的原因，因为新进入阻塞状态的线程要存入尾部节点，头结点保存了尾部节点指针，这样避免了每次尾插法都要顺序遍历一次，直接根据头结点中的尾指针就可以插入了，提高了入队效率。
在移除头结点时，下一个节点升级为head节点时能快速与尾节点关联起来。

19，读写锁，ReentrantReadWriteLock会使用两把锁来解决问题，一个读锁，一个写锁
ReentrantReadWriteLock 的核心是由一个基于AQS的同步器 Sync 构成，然后由其扩展出 ReadLock （共享锁）， WriteLock （排它锁）所组成
    线程进入读锁的前提条件：
    没有其他线程的写锁，
    没有写请求或者有写请求，但调用线程和持有锁的线程是同一个

    线程进入写锁的前提条件：
    没有其他线程的读锁
    没有其他线程的写锁
参考：https://blog.csdn.net/yupi1057/article/details/80787013，https://www.jianshu.com/p/4a624281235e

20，读写锁的使用场景：读多写少，使用此类锁同步机制则可以提高并发量（https://www.jianshu.com/p/9f98299a17a5）

21，锁降级，指的是写锁降级为读锁，实际是持有一个写锁没释放，再去申请一个读锁，再释放写锁，保留读锁，使用场景：如果当前线程不获取读锁而直接释放写锁，假设此刻另一个线程（T）获取了写锁并修改了数据，那么当前线程是无法感知线程T的数据更新，ReentrantReadWriteLock不支持锁升级

20，为啥覆盖equals 时要重写hashcode？如何重写？
    举个例子，如果重写equals方法，让对象相等，但是如果不重写hashcode，会导致使用Map结构存储数据时，会导致相等对象存储多个，也就是分布在多个hash槽
    重写参考：相同属性组成相同的hashcode

4，mq的好处：广播式解耦合，异步化处理一下长耗时逻辑，流量削峰。

5，spring mvc一次请求经历了什么（SpringMVC核心处理流程）

DispatcherServlet前端控制器接收发过来的请求，交给HandlerMapping处理器映射器

HandlerMapping处理器映射器，根据请求路径找到相应的HandlerAdapter处理器适配器（处理器适配器就是那些拦截器或Controller）

HandlerAdapter处理器适配器，处理一些功能请求，也就是真正的执行业务逻辑，返回一个ModelAndView对象（包括模型数据、逻辑视图名）

ViewResolver视图解析器，先根据ModelAndView中设置的View解析具体视图

然后再将Model模型中的数据渲染到View上

这些过程都是以DispatcherServlet为中轴线进行的。

getHandler（HandlerMapping）,获取页面处理器，通俗点就是获取由哪个Controller来执行，包含方法信息以及方法参数等信息。
getHandlerAdapter（HandlerAdapter），获取HandlerAdapter，它包含一个handle方法，负责调用真实的页面处理器进行请求处理并返回一个ModelAndView。HandlerAdpter里面有一些常见的处理，比如消息转移，参数处理等，详见此图：里面的argumentResolvers可以用来处理请求的参数，messageConverts是作消息转换等等



3，Struts和spring mvc区别。
一、拦截机制的不同
　　Struts2是类级别的拦截，每次请求就会创建一个Action，和Spring整合时Struts2的ActionBean注入作用域是原型模式prototype，然后通过setter，getter吧request数据注入到属性。Struts2中，一个Action对应一个request，response上下文，在接收参数时，可以通过属性接收，这说明属性参数是让多个方法共享的。Struts2中Action的一个方法可以对应一个url，而其类属性却被所有方法共享，这也就无法用注解或其他方式标识其所属方法了，只能设计为多例。
　　SpringMVC是方法级别的拦截，一个方法对应一个Request上下文，所以方法直接基本上是独立的，独享request，response数据。而每个方法同时又何一个url对应，参数的传递是直接注入到方法中的，是方法所独有的。处理结果通过ModeMap返回给框架。在Spring整合时，SpringMVC的Controller Bean默认单例模式Singleton，所以默认对所有的请求，只会创建一个Controller，有应为没有共享的属性，所以是线程安全的，如果要改变默认的作用域，需要添加@Scope注解修改。
　　Struts2有自己的拦截Interceptor机制，SpringMVC这是用的是独立的Aop方式，这样导致Struts2的配置文件量还是比SpringMVC大。
二、底层框架的不同
　　Struts2采用Filter（StrutsPrepareAndExecuteFilter）实现，SpringMVC（DispatcherServlet）则采用Servlet实现。Filter在容器启动之后即初始化；服务停止以后销毁，晚于Servlet。Servlet在是在调用时初始化，先于Filter调用，服务停止后销毁。
三、性能方面
　　Struts2是类级别的拦截，每次请求对应实例一个新的Action，需要加载所有的属性值注入，SpringMVC实现了零配置，由于SpringMVC基于方法的拦截（更加轻量），有加载一次单例模式bean注入。所以，SpringMVC开发效率和性能高于Struts2。



4，StackOverflowError和OutofMemoryError如何发生，怎么模拟（StackOverflowError栈溢出，如方法的递归调用，OutofMemoryError内存耗尽，比如不断创建线程分配内存）
5，jvm已经发展处三种比较成熟的垃圾收集算法：1.标记-清除算法；2.复制算法；3.标记-整理算法（标记-压缩法）；4.分代收集算法，参考：https://www.cnblogs.com/nantang/p/5674793.html
6，Jvm启动参数
一般用到最多的是
-Xms512m  设置JVM促使内存为512m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。
-Xmx512m ，设置JVM最大可用内存为512M。
-Xmn200m：设置年轻代大小为200M。整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8（young占30%左右）
7，gc，垃圾回收算法，常用的是分代收集算法（新生代和老年代分开处理），分代收集算法是复制算法和标记清除法的二者整合
8，full gc Full GC
如果某个(些)对象(原来在内存中存活的对象或者新创建的对象)由于以上原因需要被移动到老年代中，而老年代中没有足够空间容纳这个(些)对象，那么会触发一次Full GC，Full GC会对整个Heap进行一次GC，如果Full GC后还有无法给新创建的对象分配内存，或者无法移动那些需要进入老年代中的对象，那么JVM抛出OutOfMemoryError

简单理解gc
对象在Eden Space创建，当Eden Space满了的时候，gc就把所有在Eden Space中的对象扫描一次，把所有有效的对象复制到第一个Survivor Space，同时把无效的对象所占用的空间释放。当Eden Space再次变满了的时候，就启动移动程序把Eden Space中有效的对象复制到第二个Survivor Space，同时，也将第一个Survivor Space中的有效对象复制到第二个Survivor Space。如果填充到第二个Survivor Space中的有效对象被第一个Survivor Space或Eden Space中的对象引用，那么这些对象就是长期存在的，此时这些对象将被复制到Permanent Generation。若垃圾收集器依据这种小幅度的调整收集不能腾出足够的空间，就会运行Full GC，此时JVM GC停止所有在堆中运行的线程并执行清除动作。

绝大多数刚创建的对象会被分配在Eden区，其中的大多数对象很快就会消亡。Eden区是连续的内存空间，因此在其上分配内存极快；
最初一次，当Eden区满的时候，执行Minor GC，将消亡的对象清理掉，并将剩余的对象复制到一个存活区Survivor0（此时，Survivor1是空白的，两个Survivor总有一个是空白的）；
 下次Eden区满了，再执行一次Minor GC，将消亡的对象清理掉，将存活的对象复制到Survivor1中，然后清空Eden区；
 将Survivor0中消亡的对象清理掉，将其中可以晋级的对象晋级到Old区，将存活的对象也复制到Survivor1区，然后清空Survivor0区；
当两个存活区切换了几次（HotSpot虚拟机默认15次，用-XX:MaxTenuringThreshold控制，大于该值进入老年代，但这只是个最大值，并不代表一定是这个值）之后，仍然存活的对象（其实只有一小部分，比如，我们自己定义的对象），将被复制到老年代
参考：https://www.cnblogs.com/bonelee/p/8066990.html

9，cms：
PS MarkSweep：老年代收集器，是一个可以并行标记和清理垃圾的回收器，无整理，使用的是空闲列表的方式，就像一个多线程版本的Serial Old收集器
能做到老年代提前GC的垃圾回收器有CMS收集器，但它的搭配伙伴是ParNew，由ParNew（并行gc）来执行新生代垃圾回收。

CMS（Concurrent Mark Sweep）收集器：老年代收集器，致力于获取最短回收停顿时间（即缩短垃圾回收的时间），使用标记清除算法，多线程，优点是并发收集（用户线程可以和GC线程同时工作），停顿小。使用-XX:+UseConcMarkSweepGC进行ParNew+CMS+Serial Old进行内存回收，优先使用ParNew+CMS（原因见后面），当用户线程内存不足时，采用备用方案Serial Old收集

https://www.cnblogs.com/zhguang/p/3257367.html
https://www.iteye.com/topic/1119491

10，事务是必须满足4个条件（ACID）：：原子性（Atomicity，或称不可分割性）、一致性（Consistency）、隔离性（Isolation，又称独立性）、持久性（Durability）。

原子性：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。
一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。
隔离性：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。
持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失

11,java中的sleep()和wait()的区别
sleep()方法导致了程序暂停执行指定的时间，让出cpu该其他线程，但是他的监控状态依然保持者，当指定的时间到了又会自动恢复运行状态。
在调用sleep()方法的过程中，线程不会释放对象锁。
而当调用wait()方法的时候，线程会放弃对象锁，进入等待此对象的等待锁定池，只有针对此对象调用notify()方法后本线程才进入对象锁定池准备

12,重排序
编译期重排序的典型就是通过调整指令顺序，在不改变程序语义的前提下，尽可能减少寄存器的读取、存储次数，充分复用寄存器的存储值。
13，happens-before原则规则：
程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作（有依赖关系的逻辑执行先后顺序是明确知道的）；
锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作；
volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作；
传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C；
线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作；
线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生；
线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行；
对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始
14，jmm： Java内存模型是围绕着并发编程中原子性、可见性、有序性这三个特征来建立的，https://www.cnblogs.com/lewis0077/p/5143268.html
堆，方法区，本地方法区，方法栈，程序计数器。

15，Java中notify和notifyAll的区别
Java object提供了两个方法notify和notifyAll来唤醒在某些条件下等待的线程，你可以使用它们中的任何一个，但是Java中的notify和notifyAll之间存在细微差别，这使得它成为Java中流行的多线程面试问题之一。当你调用notify时，只有一个等待线程会被唤醒而且它不能保证哪个线程会被唤醒，这取决于线程调度器。虽然如果你调用notifyAll方法，那么等待该锁的所有线程都会被唤醒，但是在执行剩余的代码之前，所有被唤醒的线程都将争夺锁定，这就是为什么在循环上调用wait，因为如果多个线程被唤醒，那么线程是将获得锁定将首先执行，它可能会重置等待条件，这将迫使后续线程等待。因此，notify和notifyAll之间的关键区别在于notify（）只会唤醒一个线程，而notifyAll方法将唤醒所有线程。

wait()方法和notify()/notifyAll()方法在放弃对象监视器的时候的区别在于：wait()方法立即释放对象监视器，notify()/notifyAll()方法则会等待线程剩余代码执行完毕才会放弃对象监视器。

备注：condition的signal方法唤醒队列头部的。

16，线程共包括以下5种状态。
1. 新建状态(New)         : 线程对象被创建后，就进入了新建状态。例如，Thread thread = new Thread()。
2. 就绪状态(Runnable): 也被称为“可执行状态”。线程对象被创建后，其它线程调用了该对象的start()方法，从而来启动该线程。例如，thread.start()。处于就绪状态的线程，随时可能被CPU调度执行。
3. 运行状态(Running) : 线程获取CPU权限进行执行。需要注意的是，线程只能从就绪状态进入到运行状态。
4. 阻塞状态(Blocked)  : 阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种：
    (01) 等待阻塞 -- 通过调用线程的wait()方法，让线程等待某工作的完成。
    (02) 同步阻塞 -- 线程在获取synchronized同步锁失败(因为锁被其它线程所占用)，它会进入同步阻塞状态。
    (03) 其他阻塞 -- 通过调用线程的sleep()或join()或发出了I/O请求时，线程会进入到阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。
5. 死亡状态(Dead)    : 线程执行完了或者因异常退出了run()方法，该线程结束生命周期。
参考：https://www.cnblogs.com/happy-coder/p/6587092.html

17，synchronize保证了同步代码块内的共享变量可见性，volatile保证声明的共享变量可见性
   当一个变量定义为 volatile 之后，将具备两种特性：
　　1.保证此变量对所有的线程的可见性，这里的“可见性”，如本文开头所述，当一个线程修改了这个变量的值，volatile 保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。但普通变量做不到这点，普通变量的值在线程间传递均需要通过主内存（详见：Java内存模型）来完成。

　　2.禁止指令重排序优化。有volatile修饰的变量，赋值后多执行了一个“load addl $0x0, (%esp)”操作，这个操作相当于一个内存屏障（指令重排序时不能把后面的指令重排序到内存屏障之前的位置），只有一个CPU访问内存时，并不需要内存屏障；（什么是指令重排序：是指CPU采用了允许将多条指令不按程序规定的顺序分开发送给各相应电路单元处理。

Volatile底层实现：Lock前缀指令导致在执行指令期间，声言处理器的 LOCK# 信号。在多处理器环境中，LOCK# 信号确保在声言该信号期间，处理器可以独占使用任何共享内存。（因为它会锁住总线，导致其他CPU不能访问总线，不能访问总线就意味着不能访问系统内存），但是在最近的处理器里，LOCK＃信号一般不锁总线，而是锁缓存，毕竟锁总线开销比较大

https://blog.csdn.net/u012998254/article/details/82429333

18,synchronized的锁可重入、不可中断、非公平，而Lock锁可重入、可判断、可公平（两者皆可）, synchronized是java关键字，lock（reentrantlock）是基于cas乐观锁机制实现。
   1）Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现；
　　2）synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁；
　　3）Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断；
　　4）通过Lock可以知道有没有成功获取锁，而synchronized却无法办到。
　　5）Lock可以提高多个线程进行读操作的效率。
   6) Lock可以调用await方法让出锁资源，同时可以调用notify通知其它线程重新获取锁资源，这是synchronized不具备的
　　在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时Lock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择


19,公平锁指的是线程获取锁的顺序是按照加锁顺序来的，而非公平锁指的是抢锁机制，先lock的线程不一定先获得锁。
NonfairSync和FairSync主要就是在获取锁的方式上不同，公平锁是按顺序去获取，而非公平锁是抢占式的获取，lock的时候先去尝试修改state变量，如果抢占成功，则获取到锁。
reentrantlock的实现基于AQS（AbstractQueuedSynchronizer）实现，内部通过自旋的方式完成锁的调度，锁的实现是基于cas（compareAndSet），参考：https://www.jianshu.com/p/fadac70b2b1c。

20,ReentrantLock.lockInterruptibly允许在等待时由其它线程调用等待线程的Thread.interrupt方法来中断等待线程的等待而直接返回，这时不用获取锁，而会抛出一个InterruptedException

lock 与 lockInterruptibly比较区别在于：
lock 优先考虑获取锁，待获取锁成功后，才响应中断。（此线程在运行中， 不会收到提醒，但是此线程的 “打扰标志”会被设置， 可以通过isInterrupted()查看并作出处理）
lockInterruptibly 优先考虑响应中断，而不是响应锁的普通获取或重入获取。

可重入特性是在递归调用场景下，防止被调用过程阻塞
21，自定义锁：https://blog.csdn.net/u012545728/article/details/80843595
22，java中的基本数据类型一定存储在栈中吗？，这句话肯定是错误的。
   基本数据类型是放在栈中还是放在堆中，这取决于基本类型在何处声明，下面对数据类型在内存中的存储问题来解释一下：
   一：在方法中声明的变量，即该变量是局部变量，每当程序调用方法时，系统都会为该方法建立一个方法栈，其所在方法中声明的变量就放在方法栈中，当方法结束系统会释放方法栈，其对应在该方法中声明的变量随着栈的销毁而结束，这就局部变量只能在方法中有效的原因
      在方法中声明的变量可以是基本类型的变量，也可以是引用类型的变量。
         （1）当声明是基本类型的变量的时，其变量名及值（变量名及值是两个概念）是放在JAVA虚拟机栈中
         （2）当声明的是引用变量时，所声明的变量（该变量实际上是在方法中存储的是内存地址值）是放在JAVA虚拟机的栈中，该变量所指向的对象是放在堆类存中的。
   二：在类中声明的变量是成员变量，也叫全局变量，放在堆中的（因为全局变量不会随着某个方法执行结束而销毁）。
       同样在类中声明的变量即可是基本类型的变量 也可是引用类型的变量
       （1）当声明的是基本类型的变量其变量名及其值放在堆内存中的
       （2）引用类型时，其声明的变量仍然会存储一个内存地址值，该内存地址值指向所引用的对象。引用变量名和对应的对象仍然存储在相应的堆中


1，为什么wait()方法和notify()/notifyAll()方法要在同步块中被调用
这是JDK强制的，wait()方法和notify()/notifyAll()方法在调用前都必须先获得对象的锁

2，wait()方法和notify()/notifyAll()方法在放弃对象监视器时有什么区别
wait()方法立即释放对象监视器，notify()/notifyAll()方法则会等待线程剩余代码执行完毕才会放弃对象监视器


23，java中用到的线程调度算法是什么
抢占式。一个线程用完CPU之后，操作系统会根据线程优先级、线程饥饿情况等数据算出一个总的优先级并分配下一个时间片给某个线程执行

24，java异常

Throwable：有两个重要的子类：Exception（异常）和Error（错误），两者都包含了大量的异常处理类。

1、Error（错误）：是程序中无法处理的错误，表示运行应用程序中出现了严重的错误。此类错误一般表示代码运行时JVM出现问题。通常有Virtual MachineError（虚拟机运行错误）、NoClassDefFoundError（类定义错误）等。比如说当jvm耗完可用内存时，将出现OutOfMemoryError。此类错误发生时，JVM将终止线程。

这些错误是不可查的，非代码性错误。因此，当此类错误发生时，应用不应该去处理此类错误。

2、Exception（异常）：程序本身可以捕获并且可以处理的异常。
Exception这种异常又分为两类：运行时异常和编译异常。

1、运行时异常(不受检异常，uncheck)：RuntimeException类极其子类表示JVM在运行期间可能出现的错误。比如说试图使用空值对象的引用（NullPointerException）、数组下标越界（ArrayIndexOutBoundException）。此类异常属于不可查异常，一般是由程序逻辑错误引起的，在程序中可以选择捕获处理，也可以不处理。

2、编译异常(受检异常，check)：Exception中除RuntimeException极其子类之外的异常。如果程序中出现此类异常，比如说IOException，必须对该异常进行处理，否则编译不通过。在程序中，通常不会自定义该类异常，而是直接使用系统提供的异常类。

25,hashMap：threshold（进行扩容时所需要的判断基础，初始化为16），loadfactor是0.75，每次扩容是按照2倍扩容，扩容后threshold=table.length* loadfactor
https://www.cnblogs.com/chengxiao/p/6059914.html，https://www.cnblogs.com/peizhe123/p/5790252.html

26，ConcurrentHashMap则采用了不同的线程安全保证方式——分段锁。它不像Hashtable那样将整个table锁住而是将数组元素分段加锁，如果线程1访问的元素在分段segment1，而线程2访问的元素在分段segment2，则它们互不影响可以同时进行操作。如何合理的进行分段就是其关键问题
a, ConcurrentHashMap在数据查找的时候，为什么要两次hash？第一次hash是确定segement的位置，第二次hash是确定segement中链表的位置。
b，ConcurrentHashMap扩容，只扩容segement中的数组大小。

27，自旋锁即是某一线程去尝试获取某个锁时，如果该锁已经被其他线程占用的话，此线程将不断循环检查该锁是否被释放，而不是让此线程挂起或睡眠。它属于为了保证共享资源而提出的一种锁机制，与互斥锁类似，保证了公共资源在任意时刻最多只能由一条线程获取使用，不同的是互斥锁在获取锁失败后将进入睡眠或阻塞状态

28,Comparable和Comparator区别比较
　　Comparable是排序接口，若一个类实现了Comparable接口，就意味着“该类支持排序”。而Comparator是比较器，我们若需要控制某个类的次序，可以建立一个“该类的比较器”来进行排序。
　　Comparable相当于“内部比较器”，而Comparator相当于“外部比较器”。
　　两种方法各有优劣， 用Comparable 简单， 只要实现Comparable 接口的对象直接就成为一个可以比较的对象，但是需要修改源代码。 用Comparator 的好处是不需要修改源代码， 而是另外实现一个比较器， 当某个自定义的对象需要作比较的时候，把比较器和对象一起传递过去就可以比大小了， 并且在Comparator 里面用户可以自己实现复杂的可以通用的逻辑，使其可以匹配一些比较简单的对象，那样就可以节省很多重复劳动了。
29，如果设置线程池的大小，目前业务都是io密集型的，耗时在io，因此线程池可以设置大一些，接受更多的网络请求，常见设置是99.9线耗时（秒）*qps，即是每个线程每秒处理的请求


30，减少fullgc次数，原理上把大对象移到堆外，减少对堆空间的占用，堆空间满的时候才会触发fullgc，只有堆空间被写满的次数少了，才能减少fullgc
31，java所有的gc都会stop-the-world，包括young gc和old gc。
32，参考：https://www.cnblogs.com/yang-hao/p/5948207.html
Minor GC触发条件
     1、eden区满时，触发MinorGC。即申请一个对象时，发现eden区不够用，则触发一次MinorGC
      注：新生代分为三个区域，eden space, from space, to space。默认比例是8:1:1。在MinorGC时，会把存活的对象复制到to space区域，如果to space区域不够，则利用担保机制进入老年代区域。
     对eden space, from space, to space的理解：每次分配eden space空间，如果不够，则小于 to space大小的对象复制到 to space，然后to space和from space换位置，所以我们看到的to space一直是空的。

Full GC触发条件
老生代空间不够分配新的内存（old区不足以存放从young区复制过来的对象）

33，EHCache(Terrcotta BigMemory)的 off-heap（堆外内存，操作系统层面的堆外内存，不受gc影响）将你的对象从堆中脱离出来序列化，然后存储在一大块内存中，这就像它存储到磁盘上上一样，但它仍然在RAM中
34，Java缓存类型
2.1 堆内缓存
使用Java堆内存来存储对象。可以使用Guava Cache、Ehcache、MapDB实现。
优点：使用堆缓存的好处是没有序列化/反序列化，是最快的缓存；
缺点：很明显，当缓存的数据量很大时， GC暂停时间会变长，存储容量受限于堆空间大小；一般通过软引用/弱引用来存储缓存对象，即当堆内存不足时，可以强制回收这部分内存释放堆内存空间。一般使用堆缓存存储较热的数据。
2.2 堆外缓存
即缓存数据存储在堆外内存。可以使用Ehcache 3.x、MapDB实现。

优点：可以减少GC暂停时间(堆对象转移到堆外，GC扫描和移动的对象变少了)，可以支持更大的缓存空间(只受机器内存大小限制，不受堆空间的影响)。
缺点：读取数据时需要序列化/反序列化，会比堆缓存慢很多

34，java的本地缓存类型
1，分为堆内和堆内两种类型的缓存数据，常见的堆内缓存：如hashMap，或者guavacache，它们都收到jvm gc的影响。堆外内存有两种类型：受jvm gc影响的堆外缓存和操作系统层面的堆外缓存，受gc影响的堆外内存可以用过nio的DirectByteBuffer申请内存空间，不受gc影响的堆外内存可以通过ehcache（堆外，堆内，文件模式都支持）管理
2，DirectByteBuffer（调用unsafe的native方法申请分配内存）申请的内存空间是堆外内存，这块内存的地址会被Cleaner持有(ByteBuffer.allocateDirect，分配内存时，将这块的内存地址给Cleaner)，在gc的时候，如果这块内存空间出现无引用之后，就会被释放，也就是说这块内存空间是受到gc影响的

Cleaner类继承自PhantomReference< Object>在此处保留Cleaner对象的虚引用。此类中还包含一个静态DirectByteBuffer引用队列用于得知那些虚引用所指向的对象已回收，这是一个很棒的设计因为jvm不知道堆外内存的使用情况，通过DirectByteBuffer对象的回收来间接控制堆外内存的回收。
参考：https://blog.csdn.net/Big_Blogger/article/details/77654240


35，类加载器（http://www.importnew.com/6581.html，https://mp.weixin.qq.com/s/3LOSQnLNuaa-dEGXA-q4-w，https://www.cnblogs.com/aspirant/p/7200523.html）
类加载机制，简单理解就是委托、可见性和单一性。

<1>Bootstrap类加载器负责加载rt.jar中的JDK类文件，它是所有类加载器的父加载器

<2>Extension将加载类的请求先委托给它的父加载器，也就是Bootstrap，如果没有成功加载的话，再从jre/lib/ext目录下或者java.ext.dirs系统属性定义的目录下加载类。Extension加载器由sun.misc.Launcher$ExtClassLoader实现。这为引入除Java核心类以外的新功能提供了一个标准机制

<3>System类加载器（又叫作Application类加载器），它负责从classpath环境变量中加载某些应用相关的类，Application类加载器是Extension类加载器的子加载器。通过sun.misc.Launcher$AppClassLoader实现
<4>自定义加载器，MyClassLoader extends ClassLoader，一般只需要重写findClass（从别的地方获取类文件），最好不要重写loadClass方法，因为这样容易破坏双亲委托模式。

Java类加载器的作用就是在运行时加载类。Java类加载器基于三个机制：委托、可见性和单一性。委托机制是指将加载一个类的请求交给父类加载器，如果这个父类加载器不能够找到或者加载这个类，那么再加载它。可见性的原理是子类的加载器可以看见所有的父类加载器加载的类，而父类加载器看不到子类加载器加载的类。单一性原理是指仅加载一个类一次，这是由委托机制确保子类加载器不会再次加载父类加载器加载过的类。正确理解类加载器能够帮你解决NoClassDefFoundError和java.lang.ClassNotFoundException，因为它们和类的加载相关。类加载器通常也是比较高级的Java面试中的重要考题，Java类加载器和工作原理以及classpath如何运作的经常被问到。Java面试题中也经常出现“一个类是否能被两个不同类加载器加载”这样的问题。这篇教程中，我们将学到类加载器是什么，它的工作原理以及一些关于类加载器的知识点。

36， (1)阿里的面试官问我，可以不可以自己写个String类

答案：不可以，因为 根据类加载的双亲委派机制，会去加载父类，父类发现冲突了String就不再加载了;

(2)能否在加载类的时候，对类的字节码进行修改

答案：可以，使用Java探针技术，可以参考：Java探针-Java Agent技术-阿里面试题

(3)如何实现热部署：自定义classLoader就可以了，热部署之前，销毁（即gc回收掉）之前部署的classLoader

37，class何时触发初始化

为一个类型创建一个新的对象实例时（比如new、反射、序列化）
调用一个类型的静态方法时（即在字节码中执行invokestatic指令）
调用一个类型或接口的静态字段，或者对这些静态字段执行赋值操作时（即在字节码中，执行getstatic或者putstatic指令），不过用final修饰的静态字段除外，它被初始化为一个编译时常量表达式
调用JavaAPI中的反射方法时（比如调用java.lang.Class中的方法，或者java.lang.reflect包中其他类的方法）
初始化一个类的派生类时（Java虚拟机规范明确要求初始化一个类时，它的超类必须提前完成初始化操作，接口例外）
JVM启动包含main方法的启动类时。

38，数据库连接池简单实现，参考：https://blog.csdn.net/moakun/article/details/80690816，https://www.cnblogs.com/xdp-gacl/p/4002804.html
public class SimplePoolDemo {
    //创建一个连接池
    private static LinkedList<Connection> pool = new LinkedList<Connection>();

    //初始化10个连接
    static{
        try {
            for (int i = 0; i < 10; i++) {
                Connection conn = DBUtils.getConnection();//得到一个连接
                pool.add(conn);
            }
        } catch (Exception e) {
            throw new ExceptionInInitializerError("数据库连接失败，请检查配置");
        }
    }
    //从池中获取一个连接
    public static Connection getConnectionFromPool(){
        return pool.removeFirst();//移除一个连接对象
    }
    //释放资源
    public static void release(Connection conn){
        pool.addLast(conn);
    }
}

C3p0，dbcp，druid的区别：
c3p0有自动回收空闲连接功能，dbcp没有自动的去回收空闲连接的功能
C3P0提供最大空闲时间，DBCP提供最大连数。
Druid具备的功能更加丰富，还具备sql注入的语法校验。
参考：https://mp.weixin.qq.com/s?__biz=MzI0NjQ0MjMxNA==&mid=2247492981&idx=1&sn=12ee15db48bc41e05f2fc55aff663335

Dbcp源码解读：https://www.jianshu.com/p/f430c1d132fc和http://www.bubuko.com/infodetail-1331025.html，https://elf8848.iteye.com/blog/1931778
Dbcp源码读后总结：
dbcp有个定时器（基于定时器实现）去保证连接池维持一个称作minIdle状态（最小闲置状态，如果是无并发场景，minIdle为1就够了），大部分情况下都超过minIdle，因为数据库访问频率都很高的，当访问量增加的时候，会创建连接直至最大值为maxActive，如果连接数超过maxActive，请求会被阻塞（分为永久阻塞和限时阻塞，可配置最长等待时间maxWait）。当qps降下来，连接数不需要那么多的时候，会保持连接数在maxIdle（最大闲置数），多余的会被销毁（如果maxIdle==maxActive，就不会出现销毁了，因此生产环境一般配置maxIdle和maxActive相同）。

39，mybatis
   一级缓存和二级缓存
   1，一级缓存是SqlSession级别的缓存。在操作数据库时需要构造sqlSession对象，在对象中有一个数据结构（HashMap）用于存储缓存数据。不同的sqlSession之间的缓存数据区域（HashMap）是互相不影响的。如果中间sqlSession去执行commit操作（执行插入、更新、删除），则会清空SqlSession中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读。个人认为一级缓存不是线程安全的，对于同一行数据，另外一个线程写入数据（不使用mybatis组件），会导致session数据无法正常更新。
　　2，二级缓存是mapper级别的缓存，多个SqlSession去操作同一个Mapper的sql语句，多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的。
   3，二级缓存默认是不开启的。

40， MySQL 中的行级锁、表级锁和页级锁

InnoDB引擎默认的修改数据语句，update,delete,insert都会自动给涉及到的数据加上排他锁，select语句默认不会加任何锁类型，如果加排他锁可以使用select ...for update语句，加共享锁可以使用select ... lock in share mode语句。所以加过排他锁的数据行在其他事务种是不能修改数据的，也不能通过for update和lock in share mode锁的方式查询数据，但可以直接通过select ...from...查询数据，因为普通查询没有任何锁机制。

在数据库的锁机制中，可以按照锁的粒度把数据库锁分为行级锁（InnoDB引擎）、表级锁（MyISAM引擎）和页级锁（BDB引擎）。
行级锁：MySQL中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突，其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁和排他锁。
特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。

表级锁：MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MyISAM与InnoDB 都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。
特点：开销小，加锁快，不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。

页级锁：MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。因此，采取了折衷的页级锁，一次锁定相邻的一组记录。BDB支持页级锁。
特点：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。

MySQL 常用存储引擎的锁机制
MyISAM和Memory采用表级锁（table-level locking）
BDB采用页级锁（page-level locking）或表级锁，默认为页级锁；
InnoDB支持行级锁（row-level locking）和表级锁，默认为行级锁。

在InnoDB 引擎中既支持行锁也支持表锁，那么什么时候会锁住整张表？什么时候只锁住一行呢？
InnoDB 行锁是通过给索引上的索引项加锁来实现的，InnoDB行锁实现的特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。
在实际应用中，要特别注意 InnoDB 行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。
在不通过索引条件查询的时候，InnoDB 确实使用的是表锁，而不是行锁

行级锁又分共享锁和排他锁。
　　　　共享锁：
　　　　　　名词解释：共享锁又叫做读锁，所有的事务只能对其进行读操作不能写操作，加上共享锁后在事务结束之前其他事务只能再加共享锁，除此之外其他任何类型的锁都不能再加了。
　　　　　　用法：SELECT `id` FROM  table WHERE　id in(1,2)   LOCK IN SHARE MODE 结果集的数据都会加共享锁
　　　　排他锁：
　　　　　　名词解释：若某个事物对某一行加上了排他锁，只能这个事务对其进行读写，在此事务结束之前，其他事务不能对其进行加任何锁，其他进程可以读取,不能进行写操作，需等待其释放。
　　　　　　用法：SELECT `id` FROM mk_user WHERE id=1 FOR UPDATE

行级锁与死锁
MyISAM中是不会产生死锁的，因为MyISAM总是一次性获得所需的全部锁，要么全部满足，要么全部等待。而在InnoDB 中，锁是逐步获得的，就造成了死锁的可能。
在 MySQL 中，行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条 SQL 语句操作了主键索引，MySQL 就会锁定这条主键索引；如果一条 SQL 语句操作了非主键索引，MySQL 就会先锁定该非主键索引，再锁定相关的主键索引。 在进行UPDATE、DELETE操作时，MySQL 不仅锁定WHERE条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的next-key locking
当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引；另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。
发生死锁后，InnoDB一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。

避免死锁的方法
<1>如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低发生死锁的可能性；
<2>在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率；
<3>对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率。

行锁优化
1 尽可能让所有数据检索都通过索引来完成，避免无索引行或索引失效导致行锁升级为表锁。
2 尽可能避免间隙锁带来的性能下降，减少或使用合理的检索范围。
3 尽可能减少事务的粒度，比如控制事务大小，而从减少锁定资源量和时间长度，从而减少锁的竞争等，提供性能。
4 尽可能低级别事务隔离，隔离级别越高，并发的处理能力越低。

参考：https://blog.csdn.net/qq_35246620/article/details/69943011，https://www.cnblogs.com/deliver/p/5730616.html
41，乐观锁和悲观锁
悲观锁
总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。

乐观锁
总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。

两种锁的使用场景
从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。

42，索引失效
　　1.如果条件中有or，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因)
　　要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引
　　2.对于多列索引，不是使用的第一部分，则不会使用索引（使用组合索引没有符合最左原则，导致实际没能真实使用索引）
　　3.like查询以%开头
　　4.如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引
　　5.如果mysql估计使用全表扫描要比使用索引快,则不使用索引
   6.索引无法存储null值，因此索引中有null，查询时，采用is null条件时，不能利用到索引，只能全表扫描。
   7.not in ,not exist
参考：https://www.cnblogs.com/shynshyn/p/7887742.html，https://www.cnblogs.com/xingzc/p/5757739.html

41.内存映射技术
内存映射文件都知道,它比传统的IO读写数据快很多,那么,它为什么会这么快,从代码层面上来看,从硬盘上将文件读入内存,都是要经过数据拷贝,并且数据拷贝操作是由文件系统和硬件驱动实现的，理论上来说，拷贝数据的效率是一 样的。其实,原因是read()是系统调用，其中进行了数据 拷贝，它首先将文件内容从硬盘拷贝到内核空间的一个缓冲区，如图2中过程1，然后再将这些数据拷贝到用户空间，如图2中过程2，在这个过程中，实际上完成 了两次数据拷贝 ；而mmap()也是系统调用，如前所述，mmap()中没有进行数据拷贝，真正的数据拷贝是在缺页中断处理时进行的，由于mmap()将文件直接映射到用户空间，所以中断处理函数根据这个映射关系，直接将文件从硬盘拷贝到用户空间，只进行了 一次数据拷贝 。因此，内存映射的效率要比read/write效率高（经过测试，内存映射技术比常规的io读取，速度快上6倍左右）
（https://blog.csdn.net/whoamiyang/article/details/53365385）

JAVA NIO之浅谈内存映射文件原理与DirectMemory
在传统的文件IO操作中，我们都是调用操作系统提供的底层标准IO系统调用函数  read()、write() ，此时调用此函数的进程（在JAVA中即java进程）由当前的用户态切换到内核态，然后OS的内核代码负责将相应的文件数据读取到内核的IO缓冲区，然后再把数据从内核IO缓冲区拷贝到进程的私有地址空间中去，这样便完成了一次IO操作。至于为什么要多此一举搞一个内核IO缓冲区把原本只需一次拷贝数据的事情搞成需要2次数据拷贝呢？ 我想学过操作系统或者计算机系统结构的人都知道，这么做是为了减少磁盘的IO操作，为了提高性能而考虑的，因为我们的程序访问一般都带有局部性，也就是所谓的局部性原理，在这里主要是指的空间局部性，即我们访问了文件的某一段数据，那么接下去很可能还会访问接下去的一段数据，由于磁盘IO操作的速度比直接访问内存慢了好几个数量级，所以OS根据局部性原理会在一次 read()系统调用过程中预读更多的文件数据缓存在内核IO缓冲区中，当继续访问的文件数据在缓冲区中时便直接拷贝数据到进程私有空间，避免了再次的低效率磁盘IO操作（实际上是预读机制）

https://www.cnblogs.com/lyftest/p/6564547.html

42，kafka

典型的kafka集群中包含若干producer（可以是web前端产生的page view，或者是服务器日志，系统CPU、memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干consumer group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在consumer group发生变化时进行rebalance。producer使用push模式将消息发布到broker，consumer使用pull模式从broker订阅并消费消息

1，为了做到水平扩展，一个topic实际是由多个partition组成的（避免文件尺寸达到单机磁盘的上限，有效提升并发消费的能力），遇到瓶颈时，可以通过增加partition的数量来进行横向扩容。单个parition内是保证消息有序，同一个topic在不同partition之间是无法保证有序的（每条消息的偏移量id保证消费有序），这需要业务方根据自己的业务来实现。
2，一个broker有多个topic，每个topic的分区分散在多个broker之间，并且每个分区在各个broker有备份(即每个partition的非leader备份)，但是每个分区只有一个leader
3，异步复制，只要leader写完就算发送完成了，同步复制，得要所有follower写完才算发送完成
4，每个topic将被分成多个partition,每个partition在存储层面是append log文件。任何发布到此partition的消息都会被直接追加到log文件的尾部，每条消息在文件中的位置称为offset（偏移量），offset为一个long型数字，它是唯一标记一条消息，因此在一个partition内可以保证消息是顺序消费的。kafka并没有提供其他额外的索引机制来存储offset，因为在kafka中几乎不允许对消息进行“随机读写”。对于consumer而言,它需要保存消费消息的offset(一个partition可能对应多个多个consumer，分布在多个consume_group里面，为每个consumer分配一个offset，就可以保证线性消费了，实际上这个offset是存在zk里面，每个consumer一个offset),对于offset的保存和使用,有consumer来控制;当consumer正常消费消息时,offset将会"线性"的向前驱动,即消息将依次顺序被消费。
4，在kafka中,一个partition中的消息只会被group中的一个consumer消费
5，一个partition最多只能对应一个consumer（一个consumer可能对应多个partition）

broker: 每个正在运行的kafka节点
producer：消息生产者
consumer：消息的消费者
consumer group：消费者组，同一个消费者组只能有一个consumer能消费消息
kafka server ：也叫作broker, 已部署kafka的服务器, 以broker.id来区分不同的服务器
topic：主题, 主题中的每条消息包括key-value和timestamp。可以定义多个topic，每个topic又可以划分为多个分区
partition：topic下的消息分区，通过key取哈希后把消息映射分发到一个指定的分区，每个分区都映射到broker上的一个目录。一般每个分区存储在一个broker上
replica：副本， 每个分区按照生产者的消息达到顺序存放。每个分区副本都有一个leader
leader replica：leader角色的分区副本，leader角色的分区处理消息的读写请求. Leader和follower位于不同的broker.
follower replica：follower角色的分区副本，负责从Leader拉取数据到本地，实现分区副本的创建
zookeeper：严格来说这不是kafka的组件。但是在Kafka集群中, 很有必要通过Zookeeper管理kafka集群的配置、选举leader（每个topic对应的partition的leader），以及在Consumer Group发生变化时进行rebalance。下面说一下kafka的哪些组件需要注册到zookeeper
为什么要注册到zk集群？
1，Kafka集群通过Zookeeper来管理kafka的配置，选举leader；
2，在Consumer Group发生变化时进行rebalance
3，所有的topic与broker的对应关系都由zk维护

kafka的哪些组件需要注册到zookeeper？
（1）Broker注册到zk（因此Broker和zk之间保持心跳，如果无心跳，broker意味着挂掉）
每个broker启动时，都会注册到zk中，把自身的broker.id通知给zk。待zk创建此节点后，kafka会把这个broker的主机名和端口号记录到此节点

（2）Topic注册到zk
当broker启动时，会到对应topic节点下注册自己的broker.id到对应分区的isr列表中；当broker退出时，zk会自动更新其对应的topic分区的ISR列表，并决定是否需要做消费者的rebalance

（3）Consumer注册到zk
一旦有新的消费者组注册到zk，zk会创建专用的节点来保存相关信息。如果zk发现消费者增加或减少，会自动触发消费者的负载均衡。
（注意，producer不注册到zk）

消息如何被消费的？
Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息；producer通过联系zk获取leader角色的消息分区码，把消息写到leader
Producer使用push模式将消息发布到broker
+————+
| broker     |
+————+
   |  |
    \/
   PULL
   |  |
    \/
Consumer使用pull模式从broker订阅并消费消息

参考：https://www.cnblogs.com/liyanbin/p/7815185.html，
https://www.cnblogs.com/likehua/p/3999538.html，
https://www.jianshu.com/p/d3e963ff8b70，
https://blog.csdn.net/dshf_1/article/details/82467558


leader选举（容灾）
controller会在Zookeeper的/brokers/ids节点上注册Watch，一旦有broker宕机，它就能知道。当broker宕机后，controller就会给受到影响的partition选出新leader。controller从zk的/brokers/topics/[topic]/partitions/[partition]/state中，读取对应partition的ISR（in-sync replica已同步的副本）列表，选一个出来做leader。

kafka的数据存储
实际上是以文件的形式存储在文件系统的。topic下有partition，partition下有segment，segment是实际的一个个文件，topic和partition都是抽象概念。
在目录/${topicName}-{$partitionid}/下，存储着实际的log文件（即segment），还有对应的索引文件。
每个segment文件大小相等，文件名以这个segment中最小的offset命名，文件扩展名是.log；segment对应的索引的文件名字一样，扩展名是.index。有两个index文件，一个是offset index用于按offset去查message，一个是time index用于按照时间去查


kafka中的zookeeper作用（consumer连接到哪个broker，是由zk决定的，因此kafka的负载均衡是由zk完成的）
管理broker、consumer
创建Broker后，向zookeeper注册新的broker信息，实现在服务器正常运行下的水平拓展。具体的，通过注册watcher，获取partition的信息。
Topic的注册，zookeeper会维护topic与broker的关系，通过/brokers/topics/topic.name节点来记录。
Producer向zookeeper中注册watcher,了解topic的partition的消息，以动态了解运行情况，实现负载均衡。Zookeepr是没有管理producer，只是能够提供当前broker的相关信息。
Consumer可以使用group形式消费kafka中的数据。所有的group将以轮询的方式消费broker中的数据，具体的按照启动的顺序。Zookeeper会给每个consumer group一个ID,即同一份数据可以被不同的用户ID多次消费。因此这就是单播与多播的实现。以单个消费者还是以组别的方式去消费数据，由用户自己去定义。Zookeeper管理consumer的offset跟踪当前消费的offset.


kafka的leader选举：https://www.cnblogs.com/aspirant/p/9179045.html，https://blog.csdn.net/qq_27384769/article/details/80115392

新版本的kafka对leader的选举是这样的：在Kafka集群中会有一个或者多个broker，其中有一个broker会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态。当某个分区的leader副本出现故障时，由控制器负责为该分区选举新的leader副本。当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有broker更新其元数据信息。当使用kafka-topics.sh脚本为某个topic增加分区数量时，同样还是由控制器负责分区的重新分配

在Kafka的早期版本中，并没有采用Kafka Controller这样一个概念来对分区和副本的状态进行管理，而是依赖于Zookeeper，每个broker都会在Zookeeper上为分区和副本注册大量的监听器（Watcher）。当分区或者副本状态变化时，会唤醒很多不必要的监听器，这种严重依赖于Zookeeper的设计会有脑裂、羊群效应以及造成Zookeeper过载的隐患。在目前的新版本的设计中，只有Kafka Controller在Zookeeper上注册相应的监听器，其他的broker极少需要再监听Zookeeper中的数据变化，这样省去了很多不必要的麻烦。不过每个broker还是会对/controller节点添加监听器的，以此来监听此节点的数据变化（参考ZkClient中的IZkDataListener），简单理解就是旧版本的leader和follower partition都在zk注册，导致zk变得沉重

Zookeeper羊群效应：如果有1000个客户端watch 一个znode的exists调用，当这个节点被创建的时候，将会有1000个通知被发送。这种由于一个被watch的znode变化，导致大量的通知需要被发送，将会导致在这个通知期间的其他操作提交的延迟，因此，只要可能，我们都强烈建议不要这么使用watch。仅仅有很少的客户端同时去watch一个znode比较好，理想的情况是只有1个。
解决思路：使用zookeeper时，尽量避免大量节点监控一个节点的行为

策略就是每个client去创建一个顺序的znode /lock/lock-.ZooKeeper 会自动添加顺序号/lock/lock-xxx.我们可以通过/lock getChildren 去拿到最小的顺序号。如果client不是最小的序列号，就再比自己小一点的znode上添加watch，由于序列号最小（最先创建的node）才是获得锁的，因此去通知下一个次小的即可，不用全部通知。
参考：https://www.cnblogs.com/bnbqian/p/4846308.html，https://blog.csdn.net/nash_cyk/article/details/79139231

Kafka的消息读取为什么不从follower读取？
1，mysql不用于高qps的读取，并且允许有延迟（可以在从库读取，信息存在不准确性）。但是kafka是为了低延迟和高吞吐量，只同步到部分的follower就完成消息的投递。
2，一个consumer对应一个partition的leader，如果对应多个，就出现了一条数据重复消息多次。（除非消费一次leader，同时修改所有的follower的offset）


43，零拷贝技术（消除多余的拷贝次数，并非一次都没拷贝）
由于数据实际上仍然由磁盘复制到内存，再由内存复制到发送设备，有人可能会声称这并不是真正的"零拷贝"。然而，从操作系统的角度来看，这就是"零拷贝",因为内核空间内不存在冗余数据。应用"零拷贝"特性，出了避免复制之外，还能获得其他性能优势，例如更少的上下文切换，更少的CPU cache污染以及没有CPU必要计算校验和。

步骤一：mmap系统调用导致文件的内容通过DMA模块被复制到内核缓冲区中，该缓冲区之后与用户进程共享，这样就内核缓冲区与用户缓冲区之间的复制就不会发生。
步骤二：write系统调用导致内核将数据从内核缓冲区复制到与socket相关联的内核缓冲区中。
步骤三：DMA模块将数据由socket的缓冲区传递给协议引擎时，第3次复制发生。
参考：https://www.cnblogs.com/pengdonglin137/articles/7995528.html

44，kafka如果做到百万级高吞吐量的（参考：https://blog.csdn.net/u010039929/article/details/77934910）

生产端
1，可以将消息buffer起来,当消息的条数达到一定阀值时,批量发送给broker，即批量写入磁盘(批量写入，避免零碎写入多次io的打开和关闭)。
2，消息写入到磁盘是顺序写入，充分利用磁盘的顺序读写性能。
3，多个partition同时写入，提高数据的并发写入效率。

broker端
基于内存映射技术实现的零拷贝，sendfile系统调用可以潜在的提升网络IO的性能:将文件的数据映射到系统内存中,socket直接读取相应的内存区域即可,而无需进程再次copy和交换.
其实对于producer/consumer/broker三者而言,CPU的开支应该都不大,因此启用消息压缩机制是一个良好的策略;压缩需要消耗少量的CPU资源,不过对于kafka而言,网络IO更应该需要考虑.可以将任何在网络上传输的消息都经过压缩。
kafka支持gzip/snappy等多种压缩

消费端
批量fetch多条消息，避免多次pull操作过程中的io操作带来的性能损耗。

另外一种解读：
一、接收数据时写得快
(1)消息顺序写入磁盘
(2)消息集合批量发送
(3)采用由Producer，broker和consumer共享的标准化二进制消息格式，这样数据块就可以在它们之间自由传输，无需转换，降低了字节复制的成本开销。
(4)采用了MMAP(Memory Mapped Files，内存映射文件)技术。
(5)利用操作系统的页缓存来实现文件到物理内存的直接映射。完成映射之后对物理内存的操作在适当时候会被同步到硬盘上。
二、推送数据时发得快
(1)在生产端和消费端分别采取的push和pull的方式，也就是你生产端可以认为KAFKA是个无底洞，有多少数据可以使劲往里面推送，消费端则是根据自己的消费能力，需要多少数据，你自己过来KAFKA这里拉取，KAFKA能保证只要这里有数据，消费端需要多少，都尽可以自己过来拿。
(2)采用页缓存和sendfile组合，意味着KAFKA集群的消费者大多数都完全从缓存消费消息，而磁盘没有任何读取活动。
(3)批量压缩，支持Gzip和Snappy压缩协议。
(4)采用多分区设计，并发读写，加快读写速度。


kafka面试题：https://blog.csdn.net/linke1183982890/article/details/83303003

45，单点登录原理
单点登录全称Single Sign On（以下简称SSO），是指在多系统应用群中登录一个系统，便可在其他所有系统中得到授权而无需再次登录，包括单点登录与单点注销两部分，sso需要一个独立的认证中心，只有认证中心能接受用户的用户名密码等安全信息，其他系统不提供登录入口，只接受认证中心的间接授权。间接授权通过令牌实现，sso认证中心验证用户的用户名密码没问题，创建授权令牌，在接下来的跳转过程中，授权令牌作为参数发送给各个子系统，子系统拿到令牌，即得到了授权，可以借此创建局部会话，局部会话登录方式与单系统的登录方式相同

46，如何扩展spring
   1，自定义注解或者自定义解析器。
   2，BeanFactoryPostProcessor（调用时机是所有bean的定义信息都已经初始化好）和BeanPostProcessor（针对bean初始化提供扩展）
   3，spring aop


47，zookeeper在工作中常见的作用：它是分布式系统中的协调系统，可提供的服务主要有：配置中心（像lion）、分布式同步（分布式锁）、rpc服务注册管理（rpc注册中心）
Apache ZooKeeper是由集群（节点组）使用的一种服务，用于在自身之间协调，并通过稳健的同步技术维护共享数据。ZooKeeper本身是一个分布式应用程序，为写入分布式应用程序提供服务。

Zookeeper的角色：leader（负责进行投票的发起和决议，数据变更），follower（接受client读取请求，参与选举），observer（ZooKeeper集群的读取负载很高，可以设置一些observer服务器，以提高读取的吞吐量，不参与选举和投票），follower和Observer都是Learner。

ZooKeeper提供的常见服务如下 :
命名服务 - 按名称标识集群中的节点。它类似于DNS，但仅对于节点。
配置管理 - 加入节点的最近的和最新的系统配置信息。
集群管理 - 实时地在集群和节点状态中加入/离开节点。
选举算法 - 选举一个节点作为协调目的的leader。
锁定和同步服务 - 在修改数据的同时锁定数据。此机制可帮助你在连接其他分布式应用程序（如Apache HBase）时进行自动故障恢复。
高度可靠的数据注册表 - 即使在一个或几个节点关闭时也可以获得数据。

zookeeper入门：https://www.cnblogs.com/felixzh/p/5869212.html
zookeeper实际上以文件形式对节点进行管理，因此同一级目录下不存在重复节点

zookeeper选主流程(basic paxos)
当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的Server都恢复到一个正确的状态。Zk的选举算法有两种：一种是基于basic paxos实现的，另外一种是基于fast paxos算法实现的。系统默认的选举算法为fast paxos。

ZooKeeper如何解决"脑裂"
3种可行的思路
(1) Quorums(法定人数，一般只有过半投票才能承认选主成功)法:
通过设置法定人数, 进而确定集群的容忍度, 当集群中存活的节点少于法定人数, 集群将不可用.（或者限制全局至少一半以上节点投票才行）
比如:
3个节点的集群中, Quorums = 2 —— 集群可以容忍 (3 - 2 = 1) 个节点失败, 这时候还能选举出leader, 集群仍然可用;
4个节点的集群中, Quorums = 3 —— 集群同样可以容忍 1 个节点失败, 如果2个节点失败, 那整个集群就不可用了.
(2) Redundant communications(冗余通信):
集群中采用多种通信方式, 防止一种通信方式失效导致集群中的节点无法通信.
(3) Fencing(共享资源):
通过共享资源的方式, 将所有共享资源添加到集群中, 能对共享资源进行写操作(即加锁)的节点就是leader节点.
原文：https://blog.csdn.net/ma_shou_feng/article/details/84898305


48，zookeeper面试题（https://segmentfault.com/a/1190000014479433）
1.ZooKeeper是什么？
  ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务
2，ZooKeeper提供了什么？
 文件系统和通知机制
3，Zookeeper通知机制
client端会对某个znode建立一个watcher事件，当该znode发生变化时，这些client会收到zk的通知，然后client可以根据znode变化来做出业务上的改变等。
4，zookeeper是如何保证事务的顺序一致性的？
zookeeper采用了递增的事务Id来标识，所有的proposal（提议）都在被提出的时候加上了zxid，zxid实际上是一个64位的数字，高32位是epoch（时期; 纪元; 世; 新时代）用来标识leader是否发生改变，如果有新的leader产生出来，epoch会自增，低32位用来递增计数。当新产生proposal的时候，会依据数据库的两阶段过程，首先会向其他的server发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行
5，机器中为什么会有leader？
在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，于是就需要进行leader选举
6，部署方式？集群中的机器角色都有哪些？集群最少要几台机器
单机，集群，伪集群。Leader、Follower、Observer。集群最低3（2N+1）台，保证奇数，主要是为了选举算法。
7，集群如果有3台机器，挂掉一台集群还能工作吗？挂掉两台呢？
过半存活即可用。

8，zookeeper如何保证数据的一致性，并且如何进行leader和follower的选举？
ZooKeeper使用的是ZAB协议作为数据一致性的算法， ZAB（ZooKeeper Atomic Broadcast ） 全称为：原子消息广播协议。zab有两种工作模式：恢复模式和广播模式。zab如何保证数据的一致性：所有事务请求必须由一个全局唯一的服务器（Leader）来协调处理，其他的服务器则成为Follower。Leader会将一个客户端的事务请求转换为一个Proposal（提议），然后分发为集群中所有的Follower。当有过半数的Follower进行了正确的反馈之后，Leader会向所有的Follower发出Commit消息，然后返回客户端成功。
Zab保证数据的一致性是典型的两阶段提交策略。

选举流程：找出一批zxid最大follower，开始投票（paxos算法），投票过半的节点就会成为leader。

Zab协议特点：
1）Zab 协议需要确保那些已经在 Leader 服务器上提交（Commit）的事务最终被所有的服务器提交。
2）Zab 协议需要确保丢弃那些只在 Leader 上被提出而没有被提交的事务。

参考：https://www.jianshu.com/p/2bceacd60b8a和https://blog.csdn.net/fouy_yun/article/details/81012980，https://blog.csdn.net/a953713428/article/details/80201456


9，ZK目录树中每个节点对应一个Znode。每个Znode维护这一个属性，当前版本、数据版本（zxid）、建立时间和修改时间等。znode的数据操作是原子性的（实际还是乐观锁机制，node有版本号）。
zxid在leader和follower选举时很有意义（找出所有zxid为最大的follower，就是所有follower中数据最新的，此时它可以作为leader），zookeeper每次的节点新增、删除，数据修改都会影响zxid递增。

10，zk的节点类型，
持久节点：该数据节点被创建后，就会一直存在于zookeeper服务器上，直到有删除操作来主动删除这个节点，使用场景：常见的配置中心（如点评的lion）
临时节点：临时节点的生命周期和客户端会话绑定在一起，客户端会话失效，则这个节点就会被自动清除。使用场景：①分布式锁（客户端挂掉无法保证心跳，超时导致会话结束，临时节点会被自动删除，锁可以被其它客户端使用），②服务注册中心：服务提供者在ZooKeeper的/dubbo/com.XXX.UserService/providers节点下创建子节点（机器），服务消费者在/dubbo/com.XXX.UserService/consumers创建临时节点
顺序节点：在创建临时或者永久节点时，可以给节点加上递增编号，可以很好解决分布式锁的羊群效应，当锁释放时，只需要通知同一级别的下一个最小节点获取锁，不需要全部通知。
参考：https://segmentfault.com/a/1190000013008636，https://blog.csdn.net/wzk646795873/article/details/79706627


48，分布式锁的实现方式(https://blog.csdn.net/u010963948/article/details/79006572)
背景：分布式环境下，保证某一段业务逻辑只能被某一台机器执行
实现方式：
1，基于数据库乐观锁的版本号机制实现分布式锁（读取一行记录的版本号，根据版本号去修改，如果修改成功，就代表抢锁成功）。原理：基于行级锁实现，优点：操作简单，容易理解。缺点：不适合高并发场景。
2，基于redis的setnx实现锁创建和释放。原理：基于redis写入操作的原子性（因为redis是单线程），优点：是所有分布式锁中性能最好的（基于内存操作），缺点：由于redis的master-slave同步不是绝对可靠，可能出现锁写入主节点，还没同步到slave（在cluster集群下，客户端加锁可以强制去slave读取一遍，校验锁是否同步成功，不成功就自旋等待）
3，基于zookeeper的临时节点创建实现锁创建和释放，缺点：有羊群效应，需要调用方自己去优化逻辑。

使用redis作为分布式锁注意的问题，https://cloud.tencent.com/developer/article/1349732
举例：记得给锁加上超时时间，避免执行逻辑过程中发生异常，导致锁无法被显式释放，锁就会被永久占用，导致其它线程无法再次使用。获取锁时，最好加上自己的线程id，以便在删除锁时，再次判断是不是当前线程的锁（如果不是，需要回滚当前事务）。
备注：setnx有潜在风险，先调用setnx设置锁，再调用expire设置超时，这一连串的操作非原子操作，可能超时未设置成功就发生异常，导致引发锁无法释放，应该调用set方法，锁和超时一块执行。


49，分布式事务的解决方案
分布式事务的应用场景（soa服务化的系统里对数据的操作在不同的数据库，即保证业务数据在关联的系统中的流转正确性）
1、支付
最经典的场景就是支付了，一笔支付，是对买家账户进行扣款，同时对卖家账户进行加钱，这些操作必须在一个事务里执行，要么全部成功，要么全部失败。而对于买家账户属于买家中心，对应的是买家数据库，而卖家账户属于卖家中心，对应的是卖家数据库，对不同数据库的操作必然需要引入分布式事务。
2、在线下单
买家在电商平台下单，往往会涉及到两个动作，一个是扣库存，第二个是更新订单状态，库存和订单一般属于不同的数据库，需要使用分布式事务保证数据一致性。

解决方案
1，两阶段提交，需要中间协调器的参与，在prepare阶段，协调器向参与方（如支付宝和余额宝的相互转账）向发起请求，并且开始执行任务，所有参与方执行成功给协调器回复yes，如果所有参与方都回复yes，协调器再通知业务方commit操作（完成事务提交），否则通知所有参与方回滚。

缺点：
<1>同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。
当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。
<2>单点故障。由于协调者的重要性，一旦协调者发生故障。
参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）
<3>数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。
而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。

2，3阶段提交需要中间协调器进行协调，第一阶段协调器向所有的参与方发出执行任务的请求(cancommit)，如果回应可以，协调器通知参与方进入prepare，并执行任务。执行成功回应协调器，协调器通知所有参与方进入commit，否着回滚，中间还加入了超时机制

3.TCC模式，也是两阶段提交的一个变种。TCC提供了一个编程框架，将整个业务逻辑分为三块：Try、Confirm和Cancel三个操作。以在线下单为例，Try阶段会去扣库存，Confirm阶段则是去更新订单状态，如果更新订单失败，则进入Cancel阶段，会去恢复库存。总之，TCC就是通过代码人为实现了两阶段提交，不同的业务场景所写的代码都不一样，复杂度也不一样，因此，这种模式并不能很好地被复用。
（下单tryLock库存，confirm（下单成功），cancel（下单失败还原库存））

4，消息驱动的方式，消息方案从本质上讲是将分布式事务转换为两个本地事务，然后依靠下游业务的重试机制达到最终一致性。基于消息的最终一致性方案对应用侵入性也很高，应用需要进行大量业务改造，成本较高。（比如下单成功（预定中），消息投递给库存扣减系统，如果库存扣减失败，回调给上游的订单系统）

参考：https://www.cnblogs.com/jiangyu666/p/8522547.html，https://www.cnblogs.com/taiyonghai/p/6094350.html


49，dubbo rpc原理，参考：https://www.cnblogs.com/panxuejun/p/6094790.html
<1>服务注册、服务发现
①服务提供者
服务提供者会在初始化启动时，首先在ZooKeeper的/dubbo/com.jeiker.UserService/providers节点下创建一个子节点（机器ip），写入自己的URL地址，这就代表了“com.jeiker.UserService”这个服务的一个提供者。
②服务消费者
服务消费者会在启动时，读取并订阅ZooKeeper上/dubbo/com.jeiker.UserService/providers节点下的所有子节点（机器ip，这些机器ip会缓存在服务消费者端，防止zk挂掉，消费者仍能持续访问提供者），解析出所有提供者的URL地址来作为该服务地址列表，然后发起正常调用，同时，服务消费都还会在 ZooKeeper 的 /dubbo/com.jeiker.UserService/consumers节点下创建一个临时节点，并写入自己的URL地址，这就代表了com.jeiker.UserService这个服务的一个消费者。

<2>zookeeper在rpc框架中的模型
       			dubbo
       			 /  \
      			/    \
           com.XX.service1   com.XX.service2
               /       \
        providers     consumers
           /\           /\
10.0.11.11 10.0.11.12  10.0.11.11 10.0.11.12


参考：https://blog.csdn.net/jeikerxiao/article/details/85269139，https://segmentfault.com/a/1190000013008636

<3>Dubbo的负载均衡策略
负载均衡的实现是在客户端实现的，当服务提供方是集群的时候，为了避免大量请求一直落到一个或几个服务提供方机器上，从而使这些机器负载很高，甚至打死，需要做一定的负载均衡策略。Dubbo提供了多种均衡策略，缺省为random，也就是每次随机调用一台服务提供者的机器。
Dubbo提供的负载均衡策略
Random LoadBalance：随机策略。按照概率设置权重，比较均匀，并且可以动态调节提供者的权重。
RoundRobin LoadBalance：轮询策略。轮询，按公约后的权重设置轮询比率。会存在执行比较慢的服务提供者堆积请求的情况，比如一个机器执行的非常慢，但是机器没有挂调用（如果挂了，那么当前机器会从Zookeeper的服务列表删除），当很多新的请求到达该机器后，由于之前的请求还没有处理完毕，会导致新的请求被堆积，久而久之，所有消费者调用这台机器上的请求都被阻塞。
LeastActive LoadBalance：最少活跃调用数。如果每个提供者的活跃数相同，则随机选择一个。在每个服务提供者里面维护者一个活跃数计数器，用来记录当前同时处理请求的个数，也就是并发处理任务的个数。所以如果这个值越小说明当前服务提供者处理的速度很快或者当前机器的负载比较低，所以路由选择时候就选择该活跃度最小的机器。如果一个服务提供者处理速度很慢，由于堆积，那么同时处理的请求就比较多，也就是活跃调用数目越大，这也使得慢的提供者收到更少请求，因为越慢的提供者的活跃度越来越大。
ConsistentHash LoadBalance：一致性Hash策略。一致性Hash，可以保证相同参数的请求总是发到同一提供者，当某一台提供者挂了时，原本发往该提供者的请求，基于虚拟节点，平摊到其他提供者，不会引起剧烈变动

简单总结：常见负载均衡策略有(权重)轮询，随机，最小连接数，一致性hash等等
参考：https://www.cnblogs.com/xhj123/p/9087532.html

<4>Dubbo的核心角色
Provider: 暴露服务的服务提供方。
Consumer: 调用远程服务的服务消费方。
Registry: 服务注册与发现的注册中心。
Monitor: 统计服务的调用次调和调用时间的监控中心。

调用关系说明：
服务容器负责启动，加载，运行服务提供者。
服务提供者在启动时，向注册中心注册自己提供的服务。
服务消费者在启动时，向注册中心订阅自己所需的服务。
注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。
服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。
服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。
<5>Dubbo支持的协议

dubbo：Dubbo缺省协议是dubbo协议，采用单一长连接和NIO异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。
反之，Dubbo 缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。
rmi:RMI协议采用阻塞式(同步)短连接和JDK标准序列化方式。适用范围：传入传出参数数据包大小混合，消费者与提供者个数差不多，可传文件。
hessian:Hessian底层采用Http通讯(同步)，采用Servlet暴露服务。适用于传入传出参数数据包较大，提供者比消费者个数多，提供者压力较大，可传文件。
dubbo还支持的其他协议有：http, webservice, thrift, memcached, redis

<6>如果Dubbo的服务端未启动，消费端能起来吗?
<dubbo:reference id="" interface="" />
备注:其中reference的check默认=true，启动时会检查引用的服务是否已存在，不存在时报错，因此默认是起不来

<7>集群容错策略
正常情况下，当我们进行系统设计时候，不仅要考虑正常逻辑下代码该如何走，还要考虑异常情况下代码逻辑应该怎么走。当服务消费方调用服务提供方的服务出现错误时候，Dubbo提供了多种容错方案，缺省模式为failover，也就是失败重试。
Dubbo提供的集群容错模式：
Failover Cluster：失败重试
当服务消费方调用服务提供者失败后自动切换到其他服务提供者服务器进行重试。这通常用于读操作或者具有幂等的写操作，需要注意的是重试会带来更长延迟。可通过 retries="2" 来设置重试次数（不含第一次）。
接口级别配置重试次数方法 <dubbo:reference retries="2" /> ，如上配置当服务消费方调用服务失败后，会再重试两次，也就是说最多会做三次调用，这里的配置对该接口的所有方法生效。当然你也可以针对某个方法配置重试次数。
Failfast Cluster：快速失败
当服务消费方调用服务提供者失败后，立即报错，也就是只调用一次。通常这种模式用于非幂等性的写操作。
Failsafe Cluster：失败安全
当服务消费者调用服务出现异常时，直接忽略异常。这种模式通常用于写入审计日志等操作。
Failback Cluster：失败自动恢复
当服务消费端用服务出现异常后，在后台记录失败的请求，并按照一定的策略后期再进行重试。这种模式通常用于消息通知操作。
Forking Cluster：并行调用
当消费方调用一个接口方法后，Dubbo Client会并行调用多个服务提供者的服务，只要一个成功即返回。这种模式通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks="2" 来设置最大并行数。
Broadcast Cluster：广播调用
当消费者调用一个接口方法后，Dubbo Client会逐个调用所有服务提供者，任意一台调用异常则这次调用就标志失败。这种模式通常用于通知所有提供者更新缓存或日志等本地资源信息。
如上，Dubbo本身提供了丰富的集群容错模式，但是如果您有定制化需求，可以根据Dubbo提供的扩展接口Cluster进行定制。在后面的消费方启动流程章节会讲解何时/如何使用的集群容错。


Dubbo面试题：https://www.cnblogs.com/shan1393/p/9338530.html

50，Redis到底是多线程还是单线程？线程安全吗
redis是单线程，线程安全
redis可以能够快速执行的原因：
(1) 绝大部分请求是纯粹的内存操作（非常快速），因此瓶颈在io
(2) 采用单线程,避免了不必要的上下文切换和竞争条件
(3) 非阻塞IO - IO多路复用
redis内部实现采用epoll，采用了epoll+自己实现的简单的事件框架。epoll中的读、写、关闭、连接都转化成了事件，然后利用epoll的多路复用特性，绝不在io上浪费一点时间

51，数据库常见引擎区别

InnoDB（b+树，聚簇索引）：支持事务处理，支持外键，支持崩溃修复能力和并发控制。如果需要对事务的完整性要求比较高（比如银行），要求实现并发控制（比如售票），那选择InnoDB有很大的优势。如果需要频繁的更新、删除操作的数据库，也可以选择InnoDB，因为支持事务的提交（commit）和回滚（rollback）（用于事务处理应用程序，具有众多特性，包括ACID事务支持。(提供行级锁)）

MyISAM（b+树，非聚簇索引）：插入数据快，空间和内存使用比较低。如果表主要是用于插入新记录和读出记录，那么选择MyISAM能实现处理高效率。如果应用的完整性、并发性要求比较低，也可以使用（MyISAM类型不支持事务处理等高级处理，因此也不支持数据的回滚修复）。

MEMORY（hash结构）：所有的数据都在内存中，数据的处理速度快，但是安全性不高。如果需要很快的读写速度，对数据的安全性要求较低，可以选择MEMOEY。它对表的大小有要求，不能建立太大的表。所以，这类数据库只使用在相对较小的数据库表。

52，mysql数据库引擎的对应的索引数据结构

53，b树和b+树的比较，https://blog.csdn.net/z_ryan/article/details/79685072
1，在范围查询方面，B+树的优势更加明显，B树的范围查找需要不断依赖中序遍历。首先二分查找到范围下限，在不断通过中序遍历，知道查找到范围的上限即可。整个过程比较耗时，而B+树的范围查找则简单了许多。首先通过二分查找，找到范围下限，然后同过叶子结点的链表顺序遍历，直至找到上限即可，整个过程简单许多，效率也比较高。（b+树的数据都分布在子节点上）

54，常见二叉树
1，二叉查找树，它保证所有节点的左子树都小于该节点，所有节点的右子树都大于该节点。就可以通过大小比较关系来进行快速的检索，在一棵满二叉平衡树的情况下，检索的效率可以达到logn(类似二分检索)，然后插入和删除的效率也是稳定的logn

普通二叉查找树的一些问题：二叉搜索树是个很好的数据结构，可以快速地找到一个给定关键字的数据项，并且可以快速地插入和删除数据项。但是二叉搜索树有个很麻烦的问题，如果树中插入的是随机数据，则执行效果很好，但如果插入的是有序或者逆序的数据，那么二叉搜索树的执行速度就变得很慢。因为当插入数值有序时，二叉树就是非平衡的了，排在一条线上，其实就退化成了一个链表……它的快速查找、插入和删除指定数据项的能力就丧失了（https://blog.csdn.net/eson_15/article/details/51144079，https://www.cnblogs.com/zhuwbox/p/3636783.html）

2，AVL树不仅是一颗二叉查找树，它还有其他的性质。如果我们按照一般的二叉查找树的插入方式可能会破坏AVL树的平衡性。同理，在删除的时候也有可能会破坏树的平衡性，所以我们要做一些特殊的处理，包括：单旋转和双旋转
参考：https://www.cnblogs.com/zhuwbox/p/3636783.html
3，红黑树：https://www.cnblogs.com/chenssy/p/3746600.html，https://blog.csdn.net/qq_32924343/article/details/80856542

红黑树的特征：
1.每个节点不是红色就是黑色的；
2.根节点总是黑色的；
3.如果节点是红色的，则它的子节点必须是黑色的（反之不一定）；
4.从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度）。

使用场景：一般用于内存空间的数据排序，处理的数据量比较小（避免树的深度变的很深，导致查找深度变大），大数据排序尽量使用b+树

4，红黑树和AVL树的比较（https://www.cnblogs.com/aspirant/p/7190554.html）
1. 红黑树并不追求“完全平衡”——它只要求部分地达到平衡要求，降低了对旋转的要求，从而提高了性能。
红黑树能够以O(log2 n) 的时间复杂度进行搜索、插入、删除操作。此外，由于它的设计，任何不平衡都会在三次旋转之内解决。当然，还有一些更好的，但实现起来更复杂的数据结构，能够做到一步旋转之内达到平衡，但红黑树能够给我们一个比较“便宜”的解决方案。红黑树的算法时间复杂度和AVL相同，但统计性能比AVL树更高。
当然，红黑树并不适应所有应用树的领域。如果数据基本上是静态的，那么让他们待在他们能够插入，并且不影响平衡的地方会具有更好的性能。如果数据完全是静态的，做一个哈希表，性能可能会更好一些。
红黑树是一个更高效的检索二叉树，因此常常用来实现关联数组。典型地，JDK 提供的集合类 TreeMap 本身就是一个红黑树的实现

55，为什么Mysql用B+树做索引而不用B-树或红黑树（https://blog.csdn.net/xiedelong/article/details/81417049）
B+树只有叶节点存放数据，其余节点用来索引，而B-树是每个索引节点都会有Data域。所以从Mysql（Inoodb）的角度来看，B+树是用来充当索引的，一般来说索引非常大，尤其是关系性数据库这种数据量大的索引能达到亿级别，所以为了减少内存的占用，索引也会被存储在磁盘上。
那么Mysql如何衡量查询效率呢？– 磁盘IO次数。 B-树/B+树 的特点就是每层节点数目非常多，层数很少，目的就是为了就少磁盘IO次数，但是B-树的每个节点都有data域（指针），这无疑增大了节点大小，说白了增加了磁盘IO次数（磁盘IO一次读出的数据量大小是固定的，单个数据变大，每次读出的就少，IO次数增多，一次IO多耗时），而B+树除了叶子节点其它节点并不存储数据，节点小，磁盘IO次数就少。这是优点之一。
另一个优点是： B+树所有的Data域在叶子节点，一般来说都会进行一个优化，就是将所有的叶子节点用指针串起来。这样遍历叶子节点就能获得全部数据，这样就能进行区间访问啦。在数据库中基于范围的查询是非常频繁的，而B树不支持这样的遍历操作。

B树相对于红黑树的区别
AVL 数和红黑树基本都是存储在内存中才会使用的数据结构。在大规模数据存储的时候，红黑树往往出现由于树的深度过大而造成磁盘IO读写过于频繁，进而导致效率低下的情况。为什么会出现这样的情况，我们知道要获取磁盘上数据，必须先通过磁盘移动臂移动到数据所在的柱面，然后找到指定盘面，接着旋转盘面找到数据所在的磁道，最后对数据进行读写。磁盘IO代价主要花费在查找所需的柱面上，树的深度过大会造成磁盘IO频繁读写。根据磁盘查找存取的次数往往由树的高度所决定，所以，只要我们通过某种较好的树结构减少树的结构尽量减少树的高度，B树可以有多个子女，从几十到上千，可以降低树的高度。

因为B树是平衡树，每个节点到叶子节点的高度都是相同的，这样可以保证B树的查询是稳定的

数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧：每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。

红黑树和平衡二叉树区别如下：
1、红黑树放弃了追求完全平衡，追求大致平衡，在与平衡二叉树的时间复杂度相差不大的情况下，保证每次插入最多只需要三次旋转就能达到平衡，实现起来也更为简单。
2、平衡二叉树追求绝对平衡，条件比较苛刻，实现起来比较麻烦，每次插入新节点之后需要旋转的次数不能预知。
3，红黑树的查询性能略微逊色于AVL树，因为其比AVL树会稍微不平衡最多一层，也就是说红黑树的查询性能只比相同内容的AVL树最多多一次比较，但是，红黑树在插入和删除上优于AVL树，AVL树每次插入删除会进行大量的平衡度计算，而红黑树为了维持红黑性质所做的红黑变换和旋转的开销，相较于AVL树为了维持平衡的开销要小得多
4，红黑树的关键性质: 从根到叶子的最长的可能路径不多于（<=）最短的可能路径的两倍长。结果是这个树大致上是平衡的。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限允许红黑树在最坏情况下都是高效的，而不同于普通的二叉查找树
参考：https://www.jianshu.com/p/37436ed14cc6

总结：实际应用中，若搜索的次数远远大于插入和删除，那么选择AVL，如果搜索，插入删除次数几乎差不多，应该选择RB（red black tree）

红黑树往往出现由于树的深度过大而造成磁盘IO读写过于频繁，进而导致效率低下的情况在数据较小，可以完全放到内存中时，红黑树的时间复杂度比B树低。
如linux中进程的调度用的是红黑树，反之数据量较大，外存中占主要部分时，B树因其读磁盘次数少，而具有更快的速度。


补充：
1，由于b+树的子节点（节点结构是数组，为了二分查找提交效率）存储了所有的数据（并且各个子节点的数据是有指针关联形成的链表），因此只需要第一次递归查询就可以确定查找的起始位置，再遍历子节点就可以获取到指定范围的数据了
2，b+树和b树都是多叉树，因此可以减少因为文件过大，导致树的深度越来越深（广度换深度）。
3，AVL数和红黑树基本都是存储在内存中才会使用的数据结构（比如hashMap），整体存储的数据量少，查找数据量小，要求的性能更高。

56，hashmap，为什么用红黑树？链表（长度大于8）升级为红黑树之后，查找效率更高（二分查找），那为什么不用普通二叉查找树？红黑树本身就是二叉查找树，红黑树自身保持平衡，避免树变成了单链表，导致深度变大，查找性能降低。为什么不用b树？节点的插入效率没有红黑树高（红黑树不需要保持完全平衡，翻转次数较少，b树需要保持完全平衡，翻转次数比较多，导致耗时较长）， 为什么不用b+树？节点的插入效率没有红黑树高，数据占用的空间比较大（子节点存储所有的数据），不利于在内存中做排序。
57，跳跃表：被问到如何让链表的元素查询接近线性时间，其实类似于二叉查找树，时间复杂度是log（n）


58，二分查找（递归非递归），字符串倒序，链表倒序，链表交叉（减除多余步长，同时开始遍历）

59，mysql的聚簇索引和非聚簇索引的区别，聚簇索引（innodb）对应的b+树的子节点存储了主键索引和其它数据域（辅助索引和非索引数据），非聚簇索引对应的b+树的子节点存储了主键索引和辅助索引，对应的数据是另外存储的，非聚簇索引比聚簇索引多了一次读取数据的IO操作，所以查找性能上会差（https://blog.csdn.net/qq_27607965/article/details/79925288，https://www.cnblogs.com/0201zcr/p/5296843.html）

60，mysql的mvcc（多版本控制器）
阿里数据库内核'2017/12'月报中对MVCC的解释是:
多版本控制: 指的是一种提高并发的技术。最早的数据库系统，只有读读之间可以并发，读写，写读，写写都要阻塞。引入多版本之后，只有写写之间相互阻塞，其他三种操作都可以并行，这样大幅度提高了InnoDB的并发度。在内部实现中，与Postgres在数据行上实现多版本不同，InnoDB是在undolog中实现的，通过undolog可以找回数据的历史版本。找回的数据历史版本可以提供给用户读(按照隔离级别的定义，有些读请求只能看到比较老的数据版本)，也可以在回滚的时候覆盖数据页上的数据。在InnoDB内部中，会记录一个全局的活跃读写事务数组，其主要用来判断事务的可见性。

61，mysql，丢失更新，脏读，幻读，不可重复读，幻读（https://www.jianshu.com/p/d8bc0a843dd0）

丢失更新：由于事务A与事务B互相不知道对方的存在，导致更新不一致，解决方案：乐观锁的方式，执行修改时，加上先前获取的数据作为判断条件。
脏读：mysql中一个事务读取了另一个未提交的并行事务写的数据（可能被回滚），那这个读取就是脏读。解决方案：一个事务只能读取另一个事务已经提交的数据。
不可重复读：即不能多次重复去读，因为读出来的结果不一样，因此认为存在不可重复读的问题。解决方案：一个事务只能读取另一个事务已经提交的数据，这就会出现不可重复读的问题。
幻读：事务A一开始查询没有数据，但是插入记录失败，提示主键冲突，这种查询明明没有，插入却提示已经存在的现象，叫做幻读。解决方案：Repeatable read及以上级别通过间隙锁来防止幻读的出现，即锁定特定数据的前后间隙让数据无法被插入

小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表

62，mysql的事务隔离级别
事务隔离级别	              脏读    不可重复读	幻读
读未提交（read-uncommitted）	是	是	是
读已提交（read-committed）	否	是	是
可重复读（repeatable-read）	否	否	是
串行化（serializable）	        否	否	否

serializable级别是最高的
mysql默认的事务隔离级别为repeatable-read


63，mysql的mvcc如何解决幻读？
参考乐观锁，每开启一个事务，都获取一个版本号，后续的操作根据版本号去对比，通过版本号控制是否正常操作
InnoDB的MVCC，是通过在每行记录后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存行的过期时间（或删除时间）。当然存储的并不是实际的时间值，而是系统版本号（systemversionnumber）。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。

举例：
此时books表中有5条数据，版本号为1
事务A，系统版本号2：select * from books；因为1<=2所以此时会读取5条数据。
事务B，系统版本号3：insert into books ...，插入一条数据，新插入的数据版本号为3，而其他的数据的版本号仍然是2，插入完成之后commit，事务结束。
事务A，系统版本号2：再次select * from books；只能读取<=2的数据，事务B新插入的那条数据版本号为3，因此读不出来，解决了幻读的问题。

64，mysql四大特性
1.原子性
2.一致性
3.隔离性
4.持久性

65，mysql 七种传播行为：

1.PROPAGATION_REQUIRED：（支持事务）如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。
2.PROPAGATION_SUPPORTS：（支持事务）支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。
3.PROPAGATION_MANDATORY：（支持事务）支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。
4.PROPAGATION_REQUIRES_NEW：（支持事务）创建新事务，无论当前存不存在事务，都创建新事务。
5.PROPAGATION_NOT_SUPPORTED：（不支持事务）以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。
6.PROPAGATION_NEVER：（不支持事务）以非事务方式执行，如果当前存在事务，则抛出异常。
7.PROPAGATION_NESTED：（不支持事务）如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作

事务传播特性例子讲解：

//ServiveA
@Transactional (propagation = Propagation.REQUIRED )
public void dealA(){
    Article a = new Article();
    a.setTitlename("1");
    this.sql.insert("com.faj.dao.ArticleMapper.insert",a);//插入

    serviceB.dealB();//调用另一个Service方法
}

//ServiceB
@Transactional (propagation = Propagation.REQUIRED )
public void dealB(){
    Article a = new Article();
    a.setTitlename("3");
    this.sqlSession.insert("com.faj.dao.ArticleMapper.insert",a);
    throw new ApplicationException();
    //ApplicationException是一个继承RunTimeExcepiotn的异常处理类
}

REQUIRED是重可重入级别的，dealA调用了dealB，因为事务的传播行为是PROPAGATION_REQUIRED（如果有事务，那么加入事务，没有的话新创建一个），dealB加入到dealA的事务中，也就是两个方法共享一个事务，因为是共享事务，所以两条插入语句都会回滚。
参考：https://angelbill3.iteye.com/blog/1938478


66，mysql关联插叙方式
1，内查询（类似普通的联合查询），返回两个表的交集，例子： select * from a_table a inner join b_table b on a.a_id = b.b_id;
2，左连接（左外连接），left join 是left outer join的简写，它的全称是左外连接，是外连接中的一种。左(外)连接，左表(a_table)的记录将会全部表示出来，而右表(b_table)只会显示符合搜索条件的记录。右表记录不足的地方均为NULL。例子：select * from a_table a left join b_table bon a.a_id = b.b_id;
3，右连接（右外连接），right join是right outer join的简写，它的全称是右外连接，是外连接中的一种。与左(外)连接相反，右(外)连接，左表(a_table)只会显示符合搜索条件的记录，而右表(b_table)的记录将会全部表示出来。左表记录不足的地方均为NULL。例子：select * from a_table a right outer join b_table b on a.a_id = b.b_id;


67，连接查询（join on） 为什么比子查询（in），联合查询（from table1，table2）效率高？
表连接的时候都要先形成一张笛卡尔积表，如果两张表的数据量都比较大的话，那样就会占用很大的内存空间这显然是不合理的。所以，我们在进行表连接查询的时候一般都会使用JOIN xxx ON xxx的语法，ON语句的执行是在JOIN语句之前的，也就是说两张表数据行之间进行匹配的时候，会先判断数据行是否符合ON语句后面的条件，再决定是否JOIN，此时生成的笛卡尔积临时表比较小。避免使用 FROM table1,table2 WHERE xxx 的语法，因为会在内存中先生成一张数据量比较大的笛卡尔积表，增加了内存的开销。in子查询也是先生成临时表，再做一次笛卡尔积生成新临时表，导致效率比较低。

68，mysql索引类别
1.普通索引
2.唯一索引
3.主键索引
4.组合索引
5.全文索引（目前只有MyISAM引擎支持，对搜索引擎稍微有点了解的同学，肯定知道分词这个概念，FULLTEXT索引也是按照分词原理建立索引的。西文中，大部分为字母文字，分词可以很方便的按照空格进行分割。但很明显，中文不能按照这种方式进行分词。那又怎么办呢？这个向大家介绍一个Mysql的中文分词插件Mysqlcft）

69，什么是覆盖索引
select的数据列只用从索引中就能够取得，不必从数据表中读取，换句话说查询列要被所使用的索引覆盖。（通过explain命令可以测试是否被索引命中“返回using index”）

70，为什么选用自增量作为主键索引
   mysql的innodb和myisam是mysql最常见的两种数据库引擎，底层实现都是使用了b+树，b+树的叶子节点都是存储了有序片段数据，如果顺序自增键插入子节点更加高效，避免随机插入频繁导致节点的裂变。（非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择）

71，mysql死锁的条件及应对措施
原因：两个事务执行逻辑，各自的内部逻辑相互等待另外一个事务释放锁。

例子：
事务1
START TRANSACTION;
UPDATE StockPrice SET close = 45.50 WHERE stock_id = 4 and date = '2002-05-01';#（获取行锁）
UPDATE StockPrice SET close = 19.80 WHERE stock_id = 3 and date = '2002-05-02';#（获取行锁）
COMMIT;#释放事务里面的两个行锁
事务2
START TRANSACTION;
UPDATE StockPrice SET high = 20.12 WHERE stock_id = 3 and date = '2002-05-02';#（获取行锁）
UPDATE StockPrice SET close = 41.50 WHERE stock_id = 4 and date = '2002-05-01';#（获取行锁）
COMMIT;#释放事务里面的两个行锁

解决办法：
1，添加超时策略，超时回滚事务，避免锁被持续竞争
2，以固定的顺序访问你的表和行。则事务形成良好定义的查询并且没有死锁。
参考：https://blog.csdn.net/wwd0501/article/details/85322142

72，谈谈sql查询优化
    1，要利用索引查询。
    2，尽量利用主键查询。
    3，连接查询代（join on）替子查询（in）和联合查询（from t1，t2），避免在内存从形成巨大的笛卡尔积。
    4，批量扫表时，每次查询应该使用上次查询返回的主键id作为查询条件，提高查询效率。

73，mysql常见的集群方案（http://blog.51cto.com/mingongge/2052768）
  a，一主多从
    将master数据库中的DDL和DML操作通过二进制日志（BINLOG）传输到slave数据库上，然后将这些日志重新执行（重做）；从而使得slave数据库的数据与master数据库保持一致。
    主从复制基本原理：
    从库生成两个线程，一个I/O线程，一个SQL线程；i/o线程去请求主库 的binlog，并将得到的binlog日志写到relay log（中继日志） 文件中；主库会生成一个 log dump 线程，用来给从库 i/o线程传binlog；SQL线  程，会读取relay log文件中的日志，并解析成具体操作，来实现主从的操作一致，而最终数据一致。
参考：http://blog.51cto.com/13266497/2150272

 mysql主从复制存在的问题:
 1，主库宕机后，数据可能丢失
 2，从库只有一个sql Thread，主库写压力大，复制很可能延时

解决方法：
    半同步复制---解决数据丢失的问题
    异步复制----解决从库复制延迟的问题
全同步复制：当主库提交事务之后，所有的从库节点必须收到、APPLY并且提交这些事务，然后主库线程才能继续做后续操作。但缺点是，主库完成一个事务的时间会被拉长，性能降低。
异步复制，主库将事务 Binlog 事件写入到 Binlog 文件中，此时主库只会通知一下 Dump 线程发送这些新的 Binlog，然后主库就会继续处理提交操作，而此时不会保证这些 Binlog 传到任何一个从库节点上。
半同步复制：一个事务在主服务器上执行完成后，必须至少确保至少在一台从服务器上执行完成后，事务才算提交成功。
主库写入一个事务commit提交并执行完之后，并不直接将请求反馈给前端应用用户，而是等待从库也接收到binlog日志并成功写入中继日志后，主库才返回commit操作成功给客户端。半同步复制保障了事物执行后，至少有两份日志记录，一份在主库的binlog上 ，另一份至少在从库的中继日志Relay log上，这样就极大的保证了数据的一致性

Mysql的master和slave切换，心跳检测程序检测到master挂掉，slave之间会检测binlog的最新操作记录，确定谁是最新的就成为master，再回写到配置中心（类似zk的实现lion），并且通知业务方主库更新。

74，mysql三大范式（https://baijiahao.baidu.com/s?id=1591955163343123446&wfr=spider&for=pc）
第一范式：所有属性都不能在分解为更基本的数据单位时，简记为1NF。满足第一范式是关系数据库的最低要求。
第二范式：每个非主键属性完全依赖于主键属性，数据存在冗余，或者部分数据无法入表，如：学生表和课程表不应该整合在一块，课程信息可能冗余，也可以插不进去。
第三范式：数据不传递依赖关系（属性都跟主键有直接关系而不是间接关系），数据存在冗余，如：（学号，姓名，年龄，性别，所在院校，院校地址，院校电话）可拆成（学号，姓名，年龄，性别，所在院校）和（所在院校，院校地址，院校电话）。
75，mysql索引最左原则
   针对类似建立的索引是联合索引（a，b，c），如果查询使用a，ab，abc，ac都会调用索引查询

76，mysql组合索引的b+树结构（https://www.2cto.com/database/201802/721844.html）
联合索引(col1, col2,col3)也是一棵B+Tree，其非叶子节点存储的是第一个关键字的索引(第一列)，而叶节点存储的则是三个关键字col1、col2、col3三个关键字的数据，且按照col1、col2、col3的顺序进行排序，只有这样做才符合最左原则的查询

77，redis有哪些数据结构
    string（普通的k-v存储），
    hash，常见的对象存储，如存一个shopDTO
    list（底层实现是链表，存储重复的数据），场景：好友最新消息，基于list可以实现队列或者栈，头插或者尾插
    set（HashMap实现的，Set只用了HashMap的key列来存储对象，不允许有重复数据），集合有取交集、并集、差集等操作，因此可以求共同好友、共同兴趣、分类标签等。
    zset（有序集合，内部使用HashMap和跳跃表(SkipList)来保证数据的存储和有序），

79，redis常见面试题：https://blog.csdn.net/u010682330/article/details/81043419

80，Redis集群最大节点个数是多少？
    16384个
81，Redis相比memcached有哪些优势？
    memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型
    redis可以持久化其数据
备注：　
Memcached是多线程，非阻塞IO复用的网络模型，分为监听主线程和worker子线程，监听线程监听网络连接，接受请求后，将连接描述字pipe 传递给worker线程，进行读写IO, 网络层使用libevent封装的事件库，多线程模型可以发挥多核作用

Redis使用单线程的IO复用模型，自己封装了一个简单的AeEvent事件处理框架，主要实现了epoll、kqueue和select，对于单纯只有IO操作来说，单线程可以将速度优势发挥到最大，但是Redis也提供了一些简单的计算功能，比如排序、聚合等，对于这些操作，单线程模型实际会严重影响整体吞吐量，CPU计算过程中，整个IO调度都是被阻塞住的
参考：https://www.2cto.com/database/201809/776809.html

82，redis何时触发淘汰数据的动作

一个客户端执行指令，导致数据的增加时。
Redis检测到内存的使用已经达到上限。
Redis自身执行指令时

补充：Redis为了避免反复触发淘汰策略，每次会淘汰掉一批数据。

Redis的内存淘汰策略
Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。

83，redis内存淘汰策略（LRU算法）
allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。
allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。
volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。
volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。
volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。
noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。

Redis中同时使用了惰性过期（对cpu友好，对内存不友好，长期占用内存才能，使用的时候检查有效期），定期过期（集中使用cpu），过时立马删除（异步线程扫描所有数据），底层是有一个字典结构，在惰性删除时，会扫描检查key是否超时。

链接：https://www.jianshu.com/p/8aa619933ebb

84，MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据？
redis内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略（计算一下 20W 数据大约占用的内存，然后设置一下 Redis 内存限制即可）

85，Redis集群会有写操作丢失吗？为什么？
Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。

86、Redis集群之间是如何复制的？
异步复制

87，Reactor模式，参考：https://www.cnblogs.com/doit8791/p/7461479.html

88，Lucene倒排索引原理

倒排索引（hash表）
倒排索引在全文检索（如单词检索）环境中的利器，而mysql的b+树对于全文检索非常困难，因此mysql的b+树索引是基于数值排序（如主键索引）。

简述正排索引和倒排索引
举例：如有两个文档，doc1，doc2，里面有很多单词，我们查找单词”hello“出现的频次

正排索引实现：doc1->每个单词->词频
检索过程：先逐个遍历文档，再找单词词频，如果是海量文件，查找效率很慢（需要遍历所有的文件）

倒排索引实现：每个单词->{（doc1，10）,(doc2,13),...(文档id，频次)}
检索过程：先找单词，再找文档，再找单词词频，对于海量文件，查找效率非常高

备注：正排索引是文档id到单词，而倒排索引是单词到文档id

参考：https://www.cnblogs.com/zlslch/p/6440114.html，https://www.cnblogs.com/AndyStudy/p/9042032.html

88，Lucene倒排索引压缩算法

为了减小索引文件的大小，Lucene对索引还使用了压缩技术。
首先，对词典文件中的关键词进行了压缩，关键词压缩为<前缀长度，后缀>，例如：当前词为“阿拉伯语”，上一个词为“阿拉伯”，那么“阿拉伯语”压缩为<3，语>。
其次大量用到的是对数字的压缩，数字只保存与上一个值的差值（这样可以减小数字的长度，进而减少保存该数字需要的字节数）。例如当前文章号是16389（不压缩要用3个字节保存），上一文章号是16382，压缩后保存7（只用一个字节）。

https://www.cnblogs.com/ygj0930/articles/6586160.html

89，Lucene&Solr&ElasticSearch
<1>Elasticsearch是一个实时的分布式搜索和分析引擎。可以快速处理大规模数据。可以用于全文搜索、结构化搜索和分析
  利用zk实现分布式实时文件存储，并将每一个字段都编入索引，使其可以被搜索；
  实时分析的分布式搜索引擎；
  可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据

<2>Solr是Apache Lucene项目的开源企业搜索平台。主要功能包括全文检索、命中标示、分面搜索、动态聚类、数据库集成，以及富文本的处理。是高度可扩展的，并提供了分布式搜索和索引复制。是最流行的企业级搜索引擎。是一个独立的全文搜索服务器，solr自带分布式实现的协调器。

Solr的缺点：建立索引时，搜索效率下降，实时索引搜索效率不高（实时建立索引时，Solr会产生io阻塞，查询性能较差，Elasticsearch具有更明显的优势）。
Solr的优点：单纯的对已有数据进行搜索时，Solr更快
Solr 是传统搜索应用的有力解决方案，但 Elasticsearch 更适用于新兴的实时搜索应用。
es内对对索引构建采取了多线程，效率比solr高很多

参考：https://blog.csdn.net/Ms_lang/article/details/83215015，https://www.jianshu.com/p/7a4e1e1f56dd

90，solr和es的一些面试题
https://blog.csdn.net/Ms_lang/article/details/83215015

91，Elasticsearch集群部署（整合zk）
cluster：代表一个集群，集群中有多个节点，其中有一个为主节点，这个主节点是可以通过选举产生的，主从节点是对于集群内部来说的。es的一个概念就是去中心化，字面上理解就是无中心节点，这是对于集群外部来说的，因为从外部来看es集群，在逻辑上是个整体，你与任何一个节点的通信和与整个es集群通信是等价的。

shards：代表索引分片，es可以把一个完整的索引分成多个分片，这样的好处是可以把一个大的索引拆分成多个，分布到不同的节点上。构成分布式搜索。分片的数量只能在索引创建前指定，并且索引创建后不能更改。

replicas：代表索引副本，es可以设置多个索引的副本，副本的作用一是提高系统的容错性，当某个节点某个分片损坏或丢失时可以从副本中恢复。二是提高es的查询效率，es会自动对搜索请求进行负载均衡。

参考：https://www.cnblogs.com/aubin/p/8012840.html

92，es集群调优
https://www.cnblogs.com/guguli/p/5218297.html

93，如何提高ElasticSearch 索引速度
因为ES里大量采用线程池，构建索引的时候，是有单独的线程池做处理的，加大线程池，提高并发处理能力

94，tair原理：

一个Tair集群主要包括3个必选模块：config server、data server和client，以及一个可选模块：invalid server。

通常情况下，一个集群中包含2台config server及多台data server。两台config server互为主备并通过维护和data server之间的心跳获知集群中存活可用的data server， 构建数据在集群中的分布信息（对照表）。Data server负责数据的存储，并按照config server的指示完成数据的复制和迁移工作。 Client在启动的时候，从config server获取数据分布信息，根据数据分布信息和相应的data server交互完成用户的请求。 Invalid server主要负责对等集群的删除和隐藏操作，保证对等集群的数据一致。

从架构上看，config server的角色类似于传统应用系统的中心节点，整个集群服务依赖于config server的正常工作。但实际上相对来说， tair的config server是非常轻量级的，当正在工作的服务器宕机的时候另外一台会在秒级别时间内自动接管。 而且，如果出现两台服务器同时宕机的最恶劣情况，只要应用服务器没有新的变化，tair依然服务正常。 而有了config server这个中心节点，带来的好处就是应用在使用的时候只需要配置config server的地址（现在可以直接配置Diamond key）， 而不需要知道内部节点的情况。

ConfigServer的功能
负责管理所有的data server, 维护data server的状态信息。
通过维护和dataserver心跳来获知集群中存活节点的信息
根据存活节点的信息来构建数据在集群中的分布表。
提供数据分布表的查询服务。
调度dataserver之间的数据迁移、复制。

DataServer的功能
对外提供各种数据服务, 并以心跳的形式将自身状况汇报给config server。
提供存储引擎
接受client的put/get/remove等操作
执行数据迁移，复制等
插件：在接受请求的时候处理一些自定义功能
访问统计

InvalidServer的功能
接收来自client的invalid/hide等请求后，对属于同一组的集群（双机房独立集群部署方式）做delete/hide操作，保证同一组集群的一致。
集群断网之后的，脏数据清理。
访问统计。

client的功能
在应用端提供访问Tair集群的接口。
更新并缓存数据分布表和invalidserver地址等。
LocalCache，避免过热数据访问影响tair集群服务。
流控


参考：https://blog.csdn.net/sunjin9418/article/details/80138359


tair的特点
<1>
Tair有四种引擎：mdb, rdb, kdb和ldb。分别基于四种开源的key/value数据库：memcached, Redis, Kyoto Cabinet和leveldb。Tair可以让你更方便地使用这些KV数据库。
Tair默认包含两个存储引擎：mdb和fdb。
<2>Version支持，类似mysql的mvcc机制，支持写入数据时对版本号控制。
<3>多机架和多数据中心的支持
对照表在构建时，可以配置将数据的备份分散到不同机架或数据中心的节点上。Tair当前通过设置一个IP掩码来判断机器所属的机架和数据中心信息。
比如你配置备份数为3，集群的节点分布在两个不同的数据中心A和B，则Tair会确保每个机房至少有一份数据。假设A数据中心包含两份数据时，Tair会尽可能将这两份数据分布在不同机架的节点上。这可以减少整个数据中心或某个机架发生故障是数据丢失的风险。
<4>tair实现了一个集群多中心部署，这是redis 3.0没有实现的。
<5>DataServer负责数据的物理存储，并根据configserver构建的对照表完成数据的复制和迁移工作。DataServer具备抽象的存储引擎层，可以很方便地添加新存储引擎。DataServer还有一个插件容器，可以动态地加载/卸载插件

参考：https://www.jianshu.com/p/ccb17daed766，https://www.jianshu.com/p/ccb17daed766


tair 的负载均衡算法是什么
tair 的分布采用的是一致性哈希算法， 对于所有的key，分到Q个桶中， 桶是负载均衡和数据迁移的基本单位。 config server 根据一定的策略把每个桶指派到不同的data server上。 因为数据按照key做hash算法， 所以可以认为每个桶中的数据基本是平衡的. 保证了桶分布的均衡性， 就保证了数据分布的均衡性。

tair扩容
数据迁移时data server对外提供服务的策略，假设data server A要把桶1,2,3迁移到data server B，因为迁移完成前，客户端的路由表没有变化，客户端对1,2,3的访问请求都会路由到A，现在假设1还没迁移，2正在迁移，3已经迁移完成，那么如果访问1，则还是访问data server A，如果访问3，则A会把请求转发给B，并且将B的返回结果返回给客户，如果访问2，则在A上处理，同时如果是对2的修改操作，会记录修改log，当桶2完成迁移的时候，还有把log发送给B，在B上应用这些log，最终AB数据一致才是真正完成迁移。如果A是由于宕机而引发的迁移，客户端会收到一张中间临时状态的分配表，把宕机的data server负责的桶临时指派给有其备份的data server来处理，此时服务是可用的，负载可能不均衡，当迁移完成后，又能达到一个新的负载均衡状态

tair的数据备份：
config server会发现哪些桶的备份数目减少了， 然后根据负载情况在负载较低的data server上增加这些桶的备份，因此可以理解某个data server的数据是备份在其他的data server
如果是因为某data server宕机而引发的迁移，客户端会收到一张中间临时状态的分配表。 这张表中，把宕机的data server所负责的桶临时指派给有其备份data server来处理。这个时候，服务是可用的，但是负载可能不均衡。 当迁移完成之后，才能重新达到一个新的负载均衡的状态。


参考：https://blog.csdn.net/death_kada/article/details/48803475












